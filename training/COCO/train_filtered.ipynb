{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This file will be used to format the training set for the different tests\n",
    "\n",
    "We will begin by clearing folders that have been created by this same script. So we can get a fresh setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm  # For the progress bar\n",
    "\n",
    "# Path to the 'dataset' folder\n",
    "dataset_folder = \"dataset\"\n",
    "\n",
    "# List of folders to keep\n",
    "# folders_to_keep = [\"images\", \"labels\"]\n",
    "folders_to_keep = []\n",
    "\n",
    "# Iterate through all items in the dataset folder\n",
    "for item in os.listdir(dataset_folder):\n",
    "    item_path = os.path.join(dataset_folder, item)\n",
    "    \n",
    "    # Check if the item is not in the keep list\n",
    "    if item not in folders_to_keep:\n",
    "        # Remove the folder or file\n",
    "        if os.path.isdir(item_path):\n",
    "            shutil.rmtree(item_path)  # Remove directories\n",
    "            print(f\"Removed folder: {item_path}\")\n",
    "        else:\n",
    "            os.remove(item_path)  # Remove files\n",
    "            print(f\"Removed file: {item_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code downloads the COCO dataset from github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.downloads import download\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the dataset folder path\n",
    "dataset_dir = Path().parent / \"dataset\"  # Set dataset path relative to the script\n",
    "\n",
    "# Ensure the dataset folder exists\n",
    "dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download labels\n",
    "segments = True  # segment or box labels\n",
    "url = 'https://github.com/ultralytics/assets/releases/download/v0.0.0/'\n",
    "urls = [url + ('coco2017labels-segments.zip' if segments else 'coco2017labels.zip')]  # labels\n",
    "download(urls, dir=dataset_dir / 'labels')\n",
    "\n",
    "# Download images\n",
    "urls = [\n",
    "    'http://images.cocodataset.org/zips/train2017.zip',  # 19G, 118k images\n",
    "    'http://images.cocodataset.org/zips/val2017.zip'  # 1G, 5k images\n",
    "    # ,'http://images.cocodataset.org/zips/test2017.zip'  # 7G, 41k images (optional)\n",
    "]\n",
    "download(urls, dir=dataset_dir / 'images', threads=3)\n",
    "\n",
    "print(f\"Labels saved to {dataset_dir / 'labels'}\")\n",
    "print(f\"Images saved to {dataset_dir / 'images'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image reordering\n",
    "Copies all the images from ~/datasets/coco into the /dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define source directories\n",
    "coco_train_images = os.path.expanduser(\"dataset/images/train2017\")\n",
    "coco_val_images = os.path.expanduser(\"dataset/images/val2017\")\n",
    "coco_train_labels = os.path.expanduser(\"dataset/labels/coco/labels/train2017\")\n",
    "coco_val_labels = os.path.expanduser(\"dataset/labels/coco/labels/val2017\")\n",
    "\n",
    "# Define target directories\n",
    "dataset_images = \"dataset/images\"\n",
    "dataset_labels = \"dataset/labels\"\n",
    "\n",
    "# Ensure target directories exist\n",
    "os.makedirs(dataset_images, exist_ok=True)\n",
    "os.makedirs(dataset_labels, exist_ok=True)\n",
    "\n",
    "# Helper function to copy files\n",
    "def copy_files(source_dir, target_dir, file_extension, phase_name):\n",
    "    \"\"\"\n",
    "    Copy files with a specific extension from source_dir to target_dir.\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): Source directory path.\n",
    "        target_dir (str): Target directory path.\n",
    "        file_extension (str): File extension to filter.\n",
    "        phase_name (str): Name of the phase (e.g., 'train', 'val') for progress.\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(source_dir) if f.endswith(file_extension)]\n",
    "    \n",
    "    with tqdm(total=len(files), desc=f\"Copying {phase_name} files\", unit=\"file\") as pbar:\n",
    "        for file_name in files:\n",
    "            src_path = os.path.relpath(os.path.join(source_dir, file_name))\n",
    "            dst_path = os.path.relpath(os.path.join(target_dir, file_name))\n",
    "            shutil.move(src_path, dst_path)\n",
    "            pbar.update(1)\n",
    "\n",
    "# Copy files\n",
    "copy_files(coco_train_images, dataset_images, \".jpg\", \"Training Images\")\n",
    "copy_files(coco_val_images, dataset_images, \".jpg\", \"Validation Images\")\n",
    "copy_files(coco_train_labels, dataset_labels, \".txt\", \"Training Labels\")\n",
    "copy_files(coco_val_labels, dataset_labels, \".txt\", \"Validation Labels\")\n",
    "\n",
    "print(\"Files successfully copied to dataset/images and dataset/labels with relative paths.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear unused files that have been downloaded from the COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directories to clean\n",
    "directories = [\n",
    "    \"dataset/images\",\n",
    "    \"dataset/labels\"\n",
    "]\n",
    "\n",
    "def clean_directory(directory, extensions=[\".jpg\", \".txt\"]):\n",
    "    \"\"\"\n",
    "    Removes all files and folders from a directory except those with the specified extensions.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): The directory to clean.\n",
    "        extensions (list): The file extensions to keep (default: [\".jpg\", \".txt\"]).\n",
    "    \"\"\"\n",
    "    for item in os.listdir(directory):\n",
    "        item_path = os.path.join(directory, item)\n",
    "        # Check if the item is a file and doesn't have a desired extension\n",
    "        if os.path.isfile(item_path) and not any(item.lower().endswith(ext) for ext in extensions):\n",
    "            os.remove(item_path)  # Remove the file\n",
    "        # If the item is a folder, remove it\n",
    "        elif os.path.isdir(item_path):\n",
    "            for root, dirs, files in os.walk(item_path, topdown=False):\n",
    "                for name in files:\n",
    "                    os.remove(os.path.join(root, name))\n",
    "                for name in dirs:\n",
    "                    os.rmdir(os.path.join(root, name))\n",
    "            os.rmdir(item_path)  # Remove the directory itself\n",
    "\n",
    "# Clean each directory\n",
    "for directory in directories:\n",
    "    if os.path.exists(directory):\n",
    "        clean_directory(directory)\n",
    "        print(f\"Cleaned directory: {directory}\")\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "\n",
    "print(\"Cleaning complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat dataset to include only vehicles.\n",
    "The original dataset contains the labels of:\n",
    "\n",
    "    [\"aeroplane\", \"bicyclebike\", \"bird\", \"boat\", \"bottle\", \"bus\",\n",
    "    \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\",\n",
    "    \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "A new filtered label directory has been created so that only remain the vehicles:\n",
    "\n",
    "    [\"car\", \"bus\", \"motorbike\", \"bicyclebike\"]\n",
    "\n",
    "These new labels are stored in dataset/labels_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing YOLO label .txt files\n",
    "label_dir = \"dataset/labels\"  # Replace with your label directory path\n",
    "\n",
    "# Allowed Class IDs for vehicle-related objects\n",
    "ALLOWED_CLASSES = {0, 1, 2, 3, 4, 5, 6, 7}  # person, bicycle, car, motorcycle, airplane, bus, train, truck\n",
    "\n",
    "# Directory to save filtered labels\n",
    "output_dir = \"dataset/labels_filtered\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def filter_labels(label_file):\n",
    "    \"\"\"\n",
    "    Reads a YOLO label file, filters out unwanted classes,\n",
    "    and writes the remaining labels to a new file.\n",
    "    \"\"\"\n",
    "    input_path = os.path.join(label_dir, label_file)\n",
    "    output_path = os.path.join(output_dir, label_file)\n",
    "\n",
    "    with open(input_path, \"r\") as infile, open(output_path, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            parts = line.split()\n",
    "            class_id = int(parts[0])  # Extract class ID\n",
    "            if class_id in ALLOWED_CLASSES:\n",
    "                # Write the line if class ID is allowed\n",
    "                outfile.write(line)\n",
    "\n",
    "# List all .txt files in the label directory\n",
    "label_files = [f for f in os.listdir(label_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "# Process all .txt files with a progress bar\n",
    "with tqdm(total=len(label_files), desc=\"Filtering Labels\", unit=\"file\") as pbar:\n",
    "    for file_name in label_files:\n",
    "        filter_labels(file_name)\n",
    "        pbar.update(1)\n",
    "\n",
    "print(f\"Filtered labels saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove empty labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the labels folder\n",
    "labels_folder = \"dataset/labels_filtered\"\n",
    "\n",
    "# List all .txt files in the labels folder\n",
    "label_files = [f for f in os.listdir(labels_folder) if f.endswith('.txt')]\n",
    "\n",
    "# Initialize a counter for removed files\n",
    "removed_count = 0\n",
    "\n",
    "# Check each label file and remove it if it's empty\n",
    "for label_file in label_files:\n",
    "    label_path = os.path.join(labels_folder, label_file)\n",
    "    if os.path.getsize(label_path) == 0:  # Check if the file size is 0 bytes\n",
    "        os.remove(label_path)  # Remove the empty file\n",
    "        removed_count += 1\n",
    "        # print(f\"Removed empty label: {label_file}\")\n",
    "\n",
    "# Output the result\n",
    "print(f\"Total empty labels removed: {removed_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting images and filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "# Define the paths to the images and labels folders\n",
    "images_folder = \"dataset/images\"\n",
    "labels_folder = \"dataset/labels_filtered\"\n",
    "\n",
    "# List all files in the images and labels folders\n",
    "image_files = [f for f in os.listdir(images_folder) if f.endswith('.jpg')]\n",
    "label_files = [f for f in os.listdir(labels_folder) if f.endswith('.txt')]\n",
    "\n",
    "# Count the total number of images and labels\n",
    "num_images = len(image_files)\n",
    "num_labels = len(label_files)\n",
    "\n",
    "# Check for matching files (base filenames without extensions)\n",
    "image_basenames = {os.path.splitext(f)[0] for f in image_files}\n",
    "label_basenames = {os.path.splitext(f)[0] for f in label_files}\n",
    "\n",
    "# Count matched and unmatched files\n",
    "matched_files = image_basenames & label_basenames\n",
    "unmatched_images = image_basenames - label_basenames\n",
    "unmatched_labels = label_basenames - image_basenames\n",
    "\n",
    "print(f\"Total images: {num_images}\")\n",
    "print(f\"Total labels: {num_labels}\")\n",
    "print(f\"Matched files: {len(matched_files)}\")\n",
    "print(f\"Unmatched images: {len(unmatched_images)}\")\n",
    "print(f\"Unmatched labels: {len(unmatched_labels)}\")\n",
    "\n",
    "# Optionally print the unmatched files\n",
    "if unmatched_images:\n",
    "    print(\"Unmatched images (no corresponding label):\")\n",
    "    for img in unmatched_images:\n",
    "        print(f\"  {img}\")\n",
    "\n",
    "if unmatched_labels:\n",
    "    print(\"Unmatched labels (no corresponding image):\")\n",
    "    for lbl in unmatched_labels:\n",
    "        print(f\"  {lbl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the folder of images_filtered with a reduced number of unlabellel images.\n",
    "The ratio of labelled images and unlabelled images has been set to 50/50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Paths\n",
    "images_folder = \"dataset/images\"\n",
    "labels_folder = \"dataset/labels_filtered\"\n",
    "output_folder = \"dataset/images_filtered\"\n",
    "\n",
    "# Ratio of labeled and unlabeled images\n",
    "r_label = 50\n",
    "r_unlabel = 100 - r_label\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get all image files and corresponding label files\n",
    "image_files = [f for f in os.listdir(images_folder) if f.endswith('.jpg')]\n",
    "label_files = [f for f in os.listdir(labels_folder) if f.endswith('.txt')]\n",
    "\n",
    "# Get the base filenames (without extensions) for labels\n",
    "label_basenames = {os.path.splitext(label)[0] for label in label_files}\n",
    "\n",
    "# Separate labeled and unlabeled images\n",
    "labeled_images = [img for img in image_files if os.path.splitext(img)[0] in label_basenames]\n",
    "unlabeled_images = [img for img in image_files if os.path.splitext(img)[0] not in label_basenames]\n",
    "\n",
    "# Check counts\n",
    "num_labeled = len(labeled_images)\n",
    "num_unlabeled_to_select = min(int(num_labeled * r_unlabel / r_label), len(unlabeled_images))\n",
    "\n",
    "# Randomly select the required number of unlabeled images\n",
    "selected_unlabeled_images = random.sample(unlabeled_images, num_unlabeled_to_select)\n",
    "\n",
    "# Combine labeled and selected unlabeled images\n",
    "images_to_copy = labeled_images + selected_unlabeled_images\n",
    "\n",
    "# Copy labeled and selected unlabeled images to the output folder with a progress bar\n",
    "with tqdm(total=len(images_to_copy), desc=\"Copying Images\", unit=\"file\") as pbar:\n",
    "    for img in images_to_copy:\n",
    "        src_path = os.path.join(images_folder, img)\n",
    "        dst_path = os.path.join(output_folder, img)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Output results\n",
    "print(f\"Total labeled images: {num_labeled}\")\n",
    "print(f\"Total unlabeled images selected: {len(selected_unlabeled_images)}\")\n",
    "print(f\"Total images in 'images_filtered': {len(os.listdir(output_folder))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train, Validation and Test image sets\n",
    "From the image and labels (\"dataset/images\", \"dataset/labels_filtered\")\n",
    "Create the test, validation and test sets.\n",
    "\n",
    "\n",
    "- Training is stored in (\"dataset/train/images\", \"dataset/train/labels\")\n",
    "- Validation is stored in (\"dataset/valid/images\", \"dataset/valid/labels\")\n",
    "- Test is stored in (\"dataset/test/images\", \"dataset/test/labels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code takes the unfiltered images and create the corresponding training, validation and test set in the following folders:\n",
    "- dataset/train\n",
    "- dataset/valid\n",
    "- dataset/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Split data into 20% train, 5% validation, and 5% test\n",
    "train_perc = 0.8\n",
    "valid_perc = 0.1\n",
    "test_perc = 0.1\n",
    "\n",
    "# Define folder paths\n",
    "images_folder = \"dataset/images\"  # Folder containing filtered images\n",
    "labels_folder = \"dataset/labels\"  # Folder containing filtered labels\n",
    "\n",
    "train_images_folder = \"dataset/train/images\"\n",
    "train_labels_folder = \"dataset/train/labels\"\n",
    "\n",
    "valid_images_folder = \"dataset/valid/images\"\n",
    "valid_labels_folder = \"dataset/valid/labels\"\n",
    "\n",
    "test_images_folder = \"dataset/test/images\"\n",
    "test_labels_folder = \"dataset/test/labels\"\n",
    "\n",
    "# Create output directories\n",
    "for folder in [train_images_folder, train_labels_folder,\n",
    "               valid_images_folder, valid_labels_folder,\n",
    "               test_images_folder, test_labels_folder]:\n",
    "    if os.path.exists(folder):\n",
    "        # Remove the folder and its contents\n",
    "        shutil.rmtree(folder, ignore_errors=True)\n",
    "    \n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Get a list of all images\n",
    "image_files = sorted(os.listdir(images_folder))\n",
    "\n",
    "# Create a list of images with and without labels\n",
    "data = []\n",
    "for image_file in image_files:\n",
    "    label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "    if os.path.exists(os.path.join(labels_folder, label_file)):\n",
    "        data.append((image_file, label_file))  # Image has a corresponding label\n",
    "    else:\n",
    "        data.append((image_file, None))  # Image has no label (no objects detected)\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate splits\n",
    "total_data = len(data)\n",
    "train_split = int(train_perc * total_data)\n",
    "valid_split = train_split + int(valid_perc * total_data)\n",
    "test_split = valid_split + int(test_perc * total_data)\n",
    "\n",
    "# Allocate data\n",
    "train_data = data[:train_split]\n",
    "valid_data = data[train_split:valid_split]\n",
    "test_data = data[valid_split:test_split]\n",
    "\n",
    "# Function to copy images and labels with a progress bar\n",
    "def copy_files(data, dest_images_folder, dest_labels_folder, phase_name):\n",
    "    with tqdm(total=len(data), desc=f\"Copying {phase_name}\") as pbar:\n",
    "        for image_file, label_file in data:\n",
    "            # Copy the image file\n",
    "            shutil.copy(os.path.join(images_folder, image_file), os.path.join(dest_images_folder, image_file))\n",
    "            # Copy the label file if it exists\n",
    "            if label_file:\n",
    "                shutil.copy(os.path.join(labels_folder, label_file), os.path.join(dest_labels_folder, label_file))\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "# Copy data to respective folders\n",
    "copy_files(train_data, train_images_folder, train_labels_folder, \"Training Data\")\n",
    "copy_files(valid_data, valid_images_folder, valid_labels_folder, \"Validation Data\")\n",
    "copy_files(test_data, test_images_folder, test_labels_folder, \"Testing Data\")\n",
    "\n",
    "print(\"Dataset split complete!\")\n",
    "print(f\"Training data: {len(train_data)} images\")\n",
    "print(f\"Validation data: {len(valid_data)} images\")\n",
    "print(f\"Testing data: {len(test_data)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code takes the filtered images and create the corresponding training, validation and test set in the following folders:\n",
    "- dataset/train_filtered\n",
    "- dataset/valid_filtered\n",
    "- dataset/test_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Split data into 20% train, 5% validation, and 5% test\n",
    "train_perc = 0.8\n",
    "valid_perc = 0.1\n",
    "test_perc = 0.1\n",
    "\n",
    "# Define folder paths\n",
    "images_folder = \"dataset/images_filtered\"  # Folder containing filtered images\n",
    "labels_folder = \"dataset/labels_filtered\"  # Folder containing filtered labels\n",
    "\n",
    "train_images_folder = \"dataset/train_filtered/images\"\n",
    "train_labels_folder = \"dataset/train_filtered/labels\"\n",
    "\n",
    "valid_images_folder = \"dataset/valid_filtered/images\"\n",
    "valid_labels_folder = \"dataset/valid_filtered/labels\"\n",
    "\n",
    "test_images_folder = \"dataset/test_filtered/images\"\n",
    "test_labels_folder = \"dataset/test_filtered/labels\"\n",
    "\n",
    "# Create output directories\n",
    "for folder in [train_images_folder, train_labels_folder,\n",
    "               valid_images_folder, valid_labels_folder,\n",
    "               test_images_folder, test_labels_folder]:\n",
    "    if os.path.exists(folder):\n",
    "        # Remove the folder and its contents\n",
    "        shutil.rmtree(folder, ignore_errors=True)\n",
    "    \n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Get a list of all images\n",
    "image_files = sorted(os.listdir(images_folder))\n",
    "\n",
    "# Create a list of images with and without labels\n",
    "data = []\n",
    "for image_file in image_files:\n",
    "    label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "    if os.path.exists(os.path.join(labels_folder, label_file)):\n",
    "        data.append((image_file, label_file))  # Image has a corresponding label\n",
    "    else:\n",
    "        data.append((image_file, None))  # Image has no label (no objects detected)\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate splits\n",
    "total_data = len(data)\n",
    "train_split = int(train_perc * total_data)\n",
    "valid_split = train_split + int(valid_perc * total_data)\n",
    "test_split = valid_split + int(test_perc * total_data)\n",
    "\n",
    "# Allocate data\n",
    "train_data = data[:train_split]\n",
    "valid_data = data[train_split:valid_split]\n",
    "test_data = data[valid_split:test_split]\n",
    "\n",
    "# Function to copy images and labels with a progress bar\n",
    "def copy_files(data, dest_images_folder, dest_labels_folder, phase_name):\n",
    "    with tqdm(total=len(data), desc=f\"Copying {phase_name}\") as pbar:\n",
    "        for image_file, label_file in data:\n",
    "            # Copy the image file\n",
    "            shutil.copy(os.path.join(images_folder, image_file), os.path.join(dest_images_folder, image_file))\n",
    "            # Copy the label file if it exists\n",
    "            if label_file:\n",
    "                shutil.copy(os.path.join(labels_folder, label_file), os.path.join(dest_labels_folder, label_file))\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "# Copy data to respective folders\n",
    "copy_files(train_data, train_images_folder, train_labels_folder, \"Training Data\")\n",
    "copy_files(valid_data, valid_images_folder, valid_labels_folder, \"Validation Data\")\n",
    "copy_files(test_data, test_images_folder, test_labels_folder, \"Testing Data\")\n",
    "\n",
    "print(\"Dataset split complete!\")\n",
    "print(f\"Training data: {len(train_data)} images\")\n",
    "print(f\"Validation data: {len(valid_data)} images\")\n",
    "print(f\"Testing data: {len(test_data)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directories to process\n",
    "directories = [\n",
    "    \"dataset/labels\",\n",
    "    \"dataset/train/labels\",\n",
    "    \"dataset/valid/labels\",\n",
    "    \"dataset/test/labels\",\n",
    "    \n",
    "    \"dataset/labels_filtered\",\n",
    "    \"dataset/train_filtered/labels\",\n",
    "    \"dataset/valid_filtered/labels\",\n",
    "    \"dataset/test_filtered/labels\",\n",
    "]\n",
    "\n",
    "def count_labels_in_directory(directory):\n",
    "    \"\"\"\n",
    "    Counts the number of each class in a directory of YOLO label files with a progress bar.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing label files.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with class IDs as keys and counts as values.\n",
    "    \"\"\"\n",
    "    class_counts = defaultdict(int)\n",
    "    \n",
    "    # Get all .txt files in the directory\n",
    "    label_files = [f for f in os.listdir(directory) if f.endswith(\".txt\")]\n",
    "    \n",
    "    # Process each file with a progress bar\n",
    "    with tqdm(total=len(label_files), desc=f\"Processing {directory}\", unit=\"file\") as pbar:\n",
    "        for label_file in label_files:\n",
    "            label_path = os.path.join(directory, label_file)\n",
    "            with open(label_path, \"r\") as file:\n",
    "                for line in file:\n",
    "                    parts = line.split()\n",
    "                    class_id = int(parts[0])  # Extract class ID\n",
    "                    class_counts[class_id] += 1  # Increment the count for the class ID\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "# Count labels for each directory\n",
    "results = {}\n",
    "\n",
    "for directory in directories:\n",
    "    if os.path.exists(directory):\n",
    "        results[directory] = count_labels_in_directory(directory)\n",
    "    else:\n",
    "        results[directory] = None  # Directory does not exist\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nClass counts per directory:\")\n",
    "for directory, class_counts in results.items():\n",
    "    print(f\"\\nDirectory: {directory}\")\n",
    "    if class_counts is not None:\n",
    "        for class_id, count in sorted(class_counts.items()):\n",
    "            print(f\"  Class {class_id}: {count}\")\n",
    "    else:\n",
    "        print(\"  Directory does not exist or contains no labels.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load YOLO Model and Begin Training!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())    \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model16 = YOLO(\"yolo11n.yaml\")\n",
    "\n",
    "model16.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    project=\"./16_scratch_filtered2\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    # save_period=10,\n",
    "    # time=2,\n",
    "    # cache=\"ram\",\n",
    "    batch=16,\n",
    "    exist_ok=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "model16 = YOLO(\"16_scratch_filtered2/train/weights/last.pt\")\n",
    "model16.train(resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.67 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.66 🚀 Python-3.12.3 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m      3\u001b[0m modelX \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolo11n.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodelX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_filtered.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./X_scratch_filtered\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# save_period=10,\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# time=2,\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cache=\"ram\",\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:800\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    798\u001b[0m     args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path\n\u001b[0;32m--> 800\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:107\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[0;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplots \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 107\u001b[0m \u001b[43minit_seeds\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRANK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Dirs\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m=\u001b[39m get_save_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/ultralytics/utils/torch_utils.py:478\u001b[0m, in \u001b[0;36minit_seeds\u001b[0;34m(seed, deterministic)\u001b[0m\n\u001b[1;32m    476\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m    477\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m--> 478\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m    480\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)  \u001b[38;5;66;03m# for Multi-GPU, exception safe\u001b[39;00m\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/torch/random.py:46\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/torch/cuda/random.py:129\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    126\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    127\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 129\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:249\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 249\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/torch/cuda/random.py:127\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    126\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 127\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "modelX = YOLO(\"yolo11n.yaml\")\n",
    "modelX.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    project=\"./X_scratch_filtered\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    # save_period=10,\n",
    "    # time=2,\n",
    "    # cache=\"ram\",\n",
    "    batch=-1,\n",
    "    exist_ok=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.67 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.66 🚀 Python-3.12.3 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m      2\u001b[0m model16 \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_scratch_filtered/train/weights/last.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel16\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:800\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    798\u001b[0m     args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path\n\u001b[0;32m--> 800\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:107\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[0;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplots \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 107\u001b[0m \u001b[43minit_seeds\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRANK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Dirs\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m=\u001b[39m get_save_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/ultralytics/utils/torch_utils.py:478\u001b[0m, in \u001b[0;36minit_seeds\u001b[0;34m(seed, deterministic)\u001b[0m\n\u001b[1;32m    476\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m    477\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m--> 478\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m    480\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)  \u001b[38;5;66;03m# for Multi-GPU, exception safe\u001b[39;00m\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/torch/random.py:46\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/torch/cuda/random.py:129\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    126\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    127\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 129\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:249\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 249\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m~/ForeHelm/.venv/lib/python3.12/site-packages/torch/cuda/random.py:127\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    126\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 127\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model16 = YOLO(\"X_scratch_filtered/train/weights/last.pt\")\n",
    "model16.train(resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the YOLO models\n",
    "The following code is used to validate and compare the different YOLO models that have been trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code tries the default and pretrained yolo11n model with the COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "\n",
    "model = YOLO(\"16_scratch_filtered/train/weights/best.pt\")\n",
    "validation_results = model.val(data=\"train_filtered.yaml\", device=\"0\", plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "validation_results = model.val(data=\"train_filtered.yaml\", device=\"0\", plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "validation_results.box.map  # map50-95\n",
    "validation_results.box.map50  # map50\n",
    "validation_results.box.map75  # map75\n",
    "validation_results.box.maps  # a list contains map50-95 of each category"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
