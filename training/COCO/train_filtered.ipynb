{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This file will be used to format the training set for the different tests\n",
    "\n",
    "We will begin by clearing folders that have been created by this same script. So we can get a fresh setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\n",
      "\u001b[2K\n",
      "Ultralytics 8.3.78 ðŸš€ Python-3.11.9 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "Setup complete âœ… (22 CPUs, 31.7 GB RAM, 568.5/921.8 GB disk)\n",
      "\n",
      "OS                  Windows-10-10.0.26100-SP0\n",
      "Environment         Windows\n",
      "Python              3.11.9\n",
      "Install             pip\n",
      "RAM                 31.67 GB\n",
      "Disk                568.5/921.8 GB\n",
      "CPU                 Intel Core(TM) Ultra 9 185H\n",
      "CPU count           22\n",
      "GPU                 NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB\n",
      "GPU count           1\n",
      "CUDA                12.4\n",
      "\n",
      "numpy               âœ… 1.26.3<=2.1.1,>=1.23.0\n",
      "matplotlib          âœ… 3.10.0>=3.3.0\n",
      "opencv-python       âœ… 4.11.0.86>=4.6.0\n",
      "pillow              âœ… 10.2.0>=7.1.2\n",
      "pyyaml              âœ… 6.0.2>=5.3.1\n",
      "requests            âœ… 2.32.3>=2.23.0\n",
      "scipy               âœ… 1.15.1>=1.4.1\n",
      "torch               âœ… 2.5.1+cu124>=1.8.0\n",
      "torch               âœ… 2.5.1+cu124!=2.4.0,>=1.8.0; sys_platform == \"win32\"\n",
      "torchvision         âœ… 0.20.1+cu124>=0.9.0\n",
      "tqdm                âœ… 4.67.1>=4.64.0\n",
      "psutil              âœ… 6.1.1\n",
      "py-cpuinfo          âœ… 9.0.0\n",
      "pandas              âœ… 2.2.3>=1.1.4\n",
      "seaborn             âœ… 0.13.2>=0.11.0\n",
      "ultralytics-thop    âœ… 2.0.14>=2.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# !pip install -U ultralytics\n",
    "import ultralytics\n",
    "# from ultralytics import settings\n",
    "# # Get the current working directory\n",
    "# datasets_folder = os.path.join(os.getcwd(), \"datasets\")\n",
    "# # Update the datasets_dir setting with the actual path value\n",
    "# if not os.path.exists(datasets_folder):\n",
    "#     os.makedirs(datasets_folder)\n",
    "# settings.update({\"datasets_dir\": datasets_folder})\n",
    "!yolo checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed folder: datasets\\images\n",
      "Removed folder: datasets\\images_filtered\n",
      "Removed folder: datasets\\labels\n",
      "Removed folder: datasets\\labels_filtered\n",
      "Cleanup completed.\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Path to the 'datasets' folder\n",
    "dataset_folder = \"datasets\"\n",
    "\n",
    "# List of folders to keep (Modify this if needed)\n",
    "folders_to_keep = []  # Example: [\"images\", \"labels\"]\n",
    "\n",
    "# Check if the dataset folder exists\n",
    "if not os.path.exists(dataset_folder):\n",
    "    print(f\"Error: The folder '{dataset_folder}' does not exist.\")\n",
    "else:\n",
    "    # Iterate through all items in the dataset folder\n",
    "    for item in os.listdir(dataset_folder):\n",
    "        item_path = os.path.join(dataset_folder, item)\n",
    "        \n",
    "        # Check if the item is not in the keep list\n",
    "        if item not in folders_to_keep:\n",
    "            # Remove the folder or file\n",
    "            if os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)  # Remove directories\n",
    "                print(f\"Removed folder: {item_path}\")\n",
    "            else:\n",
    "                os.remove(item_path)  # Remove files\n",
    "                print(f\"Removed file: {item_path}\")\n",
    "print(\"Cleanup completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code downloads the COCO dataset from github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ultralytics.com/assets/coco2017labels.zip to 'datasets\\labels\\coco2017labels.zip'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46.4M/46.4M [00:02<00:00, 21.2MB/s]\n",
      "Unzipping datasets\\labels\\coco2017labels.zip to C:\\Users\\Usuario\\Documents\\TFM\\ForeHelm\\training\\COCO\\datasets\\labels\\coco...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122232/122232 [00:57<00:00, 2130.47file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://images.cocodataset.org/zips/train2017.zip to 'datasets\\images\\train2017.zip'...\n",
      "Downloading http://images.cocodataset.org/zips/val2017.zip to 'datasets\\images\\val2017.zip'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels saved to datasets\\labels\n",
      "Images saved to datasets\\images\n"
     ]
    }
   ],
   "source": [
    "from ultralytics.utils.downloads import download\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the dataset folder path\n",
    "dataset_dir = Path().parent / \"datasets\"  # Set dataset path relative to the script\n",
    "\n",
    "# Ensure the dataset folder exists\n",
    "dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download labels\n",
    "segments = False  # segment or box labels\n",
    "url = 'https://github.com/ultralytics/assets/releases/download/v0.0.0/'\n",
    "urls = [url + ('coco2017labels-segments.zip' if segments else 'coco2017labels.zip')]  # labels\n",
    "download(urls, dir=dataset_dir / 'labels')\n",
    "\n",
    "# Download images\n",
    "urls = [\n",
    "    'http://images.cocodataset.org/zips/train2017.zip',  # 19G, 118k images\n",
    "    'http://images.cocodataset.org/zips/val2017.zip'  # 1G, 5k images\n",
    "    # ,'http://images.cocodataset.org/zips/test2017.zip'  # 7G, 41k images (optional)\n",
    "]\n",
    "download(urls, dir=dataset_dir / 'images', threads=3)\n",
    "\n",
    "print(f\"Labels saved to {dataset_dir / 'labels'}\")\n",
    "print(f\"Images saved to {dataset_dir / 'images'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image reordering\n",
    "Copies all the images from ~/datasets/coco into the /dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying Training Images files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 118287/118287 [01:38<00:00, 1197.23file/s]\n",
      "Copying Validation Images files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:04<00:00, 1084.67file/s]\n",
      "Copying Training Labels files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 117266/117266 [01:24<00:00, 1383.28file/s]\n",
      "Copying Validation Labels files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:03<00:00, 1428.73file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files successfully copied to dataset/images and dataset/labels with relative paths.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define source directories\n",
    "coco_train_images = os.path.expanduser(\"datasets/images/train2017\")\n",
    "coco_val_images = os.path.expanduser(\"datasets/images/val2017\")\n",
    "coco_train_labels = os.path.expanduser(\"datasets/labels/coco/labels/train2017\")\n",
    "coco_val_labels = os.path.expanduser(\"datasets/labels/coco/labels/val2017\")\n",
    "\n",
    "# Define target directories\n",
    "dataset_images = \"datasets/images\"\n",
    "dataset_labels = \"datasets/labels\"\n",
    "\n",
    "# Ensure target directories exist\n",
    "os.makedirs(dataset_images, exist_ok=True)\n",
    "os.makedirs(dataset_labels, exist_ok=True)\n",
    "\n",
    "# Helper function to copy files\n",
    "def copy_files(source_dir, target_dir, file_extension, phase_name):\n",
    "    \"\"\"\n",
    "    Copy files with a specific extension from source_dir to target_dir.\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): Source directory path.\n",
    "        target_dir (str): Target directory path.\n",
    "        file_extension (str): File extension to filter.\n",
    "        phase_name (str): Name of the phase (e.g., 'train', 'val') for progress.\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(source_dir) if f.endswith(file_extension)]\n",
    "    \n",
    "    with tqdm(total=len(files), desc=f\"Copying {phase_name} files\", unit=\"file\") as pbar:\n",
    "        for file_name in files:\n",
    "            src_path = os.path.relpath(os.path.join(source_dir, file_name))\n",
    "            dst_path = os.path.relpath(os.path.join(target_dir, file_name))\n",
    "            shutil.move(src_path, dst_path)\n",
    "            pbar.update(1)\n",
    "\n",
    "# Copy files\n",
    "copy_files(coco_train_images, dataset_images, \".jpg\", \"Training Images\")\n",
    "copy_files(coco_val_images, dataset_images, \".jpg\", \"Validation Images\")\n",
    "copy_files(coco_train_labels, dataset_labels, \".txt\", \"Training Labels\")\n",
    "copy_files(coco_val_labels, dataset_labels, \".txt\", \"Validation Labels\")\n",
    "\n",
    "print(\"Files successfully copied to dataset/images and dataset/labels with relative paths.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear unused files that have been downloaded from the COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned directory: datasets/images\n",
      "Cleaned directory: datasets/labels\n",
      "Cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directories to clean\n",
    "directories = [\n",
    "    \"datasets/images\",\n",
    "    \"datasets/labels\"\n",
    "]\n",
    "\n",
    "def clean_directory(directory, extensions=[\".jpg\", \".txt\"]):\n",
    "    \"\"\"\n",
    "    Removes all files and folders from a directory except those with the specified extensions.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): The directory to clean.\n",
    "        extensions (list): The file extensions to keep (default: [\".jpg\", \".txt\"]).\n",
    "    \"\"\"\n",
    "    for item in os.listdir(directory):\n",
    "        item_path = os.path.join(directory, item)\n",
    "        # Check if the item is a file and doesn't have a desired extension\n",
    "        if os.path.isfile(item_path) and not any(item.lower().endswith(ext) for ext in extensions):\n",
    "            os.remove(item_path)  # Remove the file\n",
    "        # If the item is a folder, remove it\n",
    "        elif os.path.isdir(item_path):\n",
    "            for root, dirs, files in os.walk(item_path, topdown=False):\n",
    "                for name in files:\n",
    "                    os.remove(os.path.join(root, name))\n",
    "                for name in dirs:\n",
    "                    os.rmdir(os.path.join(root, name))\n",
    "            os.rmdir(item_path)  # Remove the directory itself\n",
    "\n",
    "# Clean each directory\n",
    "for directory in directories:\n",
    "    if os.path.exists(directory):\n",
    "        clean_directory(directory)\n",
    "        print(f\"Cleaned directory: {directory}\")\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "\n",
    "print(\"Cleaning complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat dataset to include only vehicles.\n",
    "The original dataset contains the labels of:\n",
    "\n",
    "    [\"aeroplane\", \"bicyclebike\", \"bird\", \"boat\", \"bottle\", \"bus\",\n",
    "    \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\",\n",
    "    \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "A new filtered label directory has been created so that only remain the vehicles:\n",
    "\n",
    "    [\"car\", \"bus\", \"motorbike\", \"bicyclebike\"]\n",
    "\n",
    "These new labels are stored in dataset/labels_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering & Mapping Labels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122218/122218 [11:13<00:00, 181.40file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered and mapped labels saved in: datasets/labels_filtered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directory containing YOLO label .txt files\n",
    "label_dir = \"datasets/labels\"  # Replace with your label directory path\n",
    "\n",
    "# Mapping of original class IDs to new class IDs\n",
    "CLASS_MAPPING = {0: 0, 1: 1, 2: 2, 3: 3, 5: 4, 6: 5, 7: 6}\n",
    "\n",
    "# Directory to save filtered labels\n",
    "output_dir = \"datasets/labels_filtered\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def filter_and_map_labels(label_file):\n",
    "    \"\"\"\n",
    "    Reads a YOLO label file, filters out unwanted classes,\n",
    "    maps allowed classes to new values, and writes the updated labels to a new file.\n",
    "    \"\"\"\n",
    "    input_path = os.path.join(label_dir, label_file)\n",
    "    output_path = os.path.join(output_dir, label_file)\n",
    "\n",
    "    with open(input_path, \"r\") as infile, open(output_path, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            parts = line.split()\n",
    "            class_id = int(parts[0])  # Extract class ID\n",
    "\n",
    "            if class_id in CLASS_MAPPING:\n",
    "                new_class_id = CLASS_MAPPING[class_id]\n",
    "                # Replace class ID and write the updated line\n",
    "                outfile.write(f\"{new_class_id} \" + \" \".join(parts[1:]) + \"\\n\")\n",
    "\n",
    "# List all .txt files in the label directory\n",
    "label_files = [f for f in os.listdir(label_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "# Process all .txt files with a progress bar\n",
    "with tqdm(total=len(label_files), desc=\"Filtering & Mapping Labels\", unit=\"file\") as pbar:\n",
    "    for file_name in label_files:\n",
    "        filter_and_map_labels(file_name)\n",
    "        pbar.update(1)\n",
    "\n",
    "print(f\"Filtered and mapped labels saved in: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove empty labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total empty labels removed: 47434\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the labels folder\n",
    "labels_folder = \"datasets/labels_filtered\"\n",
    "\n",
    "# List all .txt files in the labels folder\n",
    "label_files = [f for f in os.listdir(labels_folder) if f.endswith('.txt')]\n",
    "\n",
    "# Initialize a counter for removed files\n",
    "removed_count = 0\n",
    "\n",
    "# Check each label file and remove it if it's empty\n",
    "for label_file in label_files:\n",
    "    label_path = os.path.join(labels_folder, label_file)\n",
    "    if os.path.getsize(label_path) == 0:  # Check if the file size is 0 bytes\n",
    "        os.remove(label_path)  # Remove the empty file\n",
    "        removed_count += 1\n",
    "        # print(f\"Removed empty label: {label_file}\")\n",
    "\n",
    "# Output the result\n",
    "print(f\"Total empty labels removed: {removed_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting images and filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.getcwd())\n",
    "# # Define the paths to the images and labels folders\n",
    "# images_folder = \"datasets/images\"\n",
    "# labels_folder = \"datasets/labels_filtered\"\n",
    "\n",
    "# # List all files in the images and labels folders\n",
    "# image_files = [f for f in os.listdir(images_folder) if f.endswith('.jpg')]\n",
    "# label_files = [f for f in os.listdir(labels_folder) if f.endswith('.txt')]\n",
    "\n",
    "# # Count the total number of images and labels\n",
    "# num_images = len(image_files)\n",
    "# num_labels = len(label_files)\n",
    "\n",
    "# # Check for matching files (base filenames without extensions)\n",
    "# image_basenames = {os.path.splitext(f)[0] for f in image_files}\n",
    "# label_basenames = {os.path.splitext(f)[0] for f in label_files}\n",
    "\n",
    "# # Count matched and unmatched files\n",
    "# matched_files = image_basenames & label_basenames\n",
    "# unmatched_images = image_basenames - label_basenames\n",
    "# unmatched_labels = label_basenames - image_basenames\n",
    "\n",
    "# print(f\"Total images: {num_images}\")\n",
    "# print(f\"Total labels: {num_labels}\")\n",
    "# print(f\"Matched files: {len(matched_files)}\")\n",
    "# print(f\"Unmatched images: {len(unmatched_images)}\")\n",
    "# print(f\"Unmatched labels: {len(unmatched_labels)}\")\n",
    "\n",
    "# # Optionally print the unmatched files\n",
    "# # if unmatched_images:\n",
    "# #     print(\"Unmatched images (no corresponding label):\")\n",
    "# #     for img in unmatched_images:\n",
    "# #         # print(f\"  {img}\")\n",
    "\n",
    "# # if unmatched_labels:\n",
    "# #     print(\"Unmatched labels (no corresponding image):\")\n",
    "# #     for lbl in unmatched_labels:\n",
    "# #         # print(f\"  {lbl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the folder of images_filtered with a reduced number of unlabellel images.\n",
    "The ratio of labelled images and unlabelled images has been set to 70/30. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123287/123287 [03:09<00:00, 650.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Copied 123287 images to 'datasets/images_filtered'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define paths\n",
    "images_folder = \"datasets/images\"\n",
    "output_folder = \"datasets/images_filtered\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get list of images (assuming .jpg format, modify if needed)\n",
    "image_files = [f for f in os.listdir(images_folder) if f.endswith(\".jpg\")]\n",
    "\n",
    "# Copy images to the output folder\n",
    "for image_file in tqdm(image_files, desc=\"Copying Images\"):\n",
    "    src_path = os.path.join(images_folder, image_file)\n",
    "    dst_path = os.path.join(output_folder, image_file)\n",
    "    shutil.copy2(src_path, dst_path)  # Preserve metadata\n",
    "\n",
    "print(f\"âœ… Copied {len(image_files)} images to '{output_folder}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Class Frequencies: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74784/74784 [06:54<00:00, 180.35file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common class: 0 (Threshold: 45451)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering Dominant Class Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74784/74784 [00:19<00:00, 3934.65file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removed 51048 images where only class 0 was present.\n",
      "Final count of class 0: 216981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Paths\n",
    "images_folder = \"datasets/images_filtered\"\n",
    "labels_folder = \"datasets/labels_filtered\"\n",
    "\n",
    "# Step 1: Count class occurrences across all label files\n",
    "class_counts = defaultdict(int)\n",
    "file_classes = {}  # Store classes per file\n",
    "\n",
    "# Get label files\n",
    "label_files = [f for f in os.listdir(labels_folder) if f.endswith(\".txt\")]\n",
    "\n",
    "for label_file in tqdm(label_files, desc=\"Counting Class Frequencies\", unit=\"file\"):\n",
    "    label_path = os.path.join(labels_folder, label_file)\n",
    "    with open(label_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Store class IDs present in this file\n",
    "    file_classes[label_file] = set()\n",
    "    \n",
    "    for line in lines:\n",
    "        class_id = int(line.split()[0])  # Extract class ID\n",
    "        class_counts[class_id] += 1\n",
    "        file_classes[label_file].add(class_id)\n",
    "\n",
    "# Step 2: Identify dominant class and second most common class\n",
    "sorted_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "most_common_class = sorted_classes[0][0]  # Most frequent class\n",
    "second_most_common_class_count = sorted_classes[1][1]  # Count of second most frequent class\n",
    "\n",
    "# Define threshold for most common class\n",
    "threshold = second_most_common_class_count\n",
    "print(f\"\\nMost common class: {most_common_class} (Threshold: {threshold})\")\n",
    "\n",
    "# Step 3: Remove images where only the most common class exists\n",
    "removed_files = 0\n",
    "for label_file, classes in tqdm(file_classes.items(), desc=\"Filtering Dominant Class Images\", unit=\"file\"):\n",
    "    if most_common_class in classes and len(classes) == 1:  # Only contains the dominant class\n",
    "        # Remove corresponding image and label\n",
    "        label_path = os.path.join(labels_folder, label_file)\n",
    "        image_path = os.path.join(images_folder, label_file.replace(\".txt\", \".jpg\"))  # Assuming .jpg images\n",
    "        \n",
    "        os.remove(label_path)\n",
    "        os.remove(image_path)\n",
    "        \n",
    "        removed_files += 1\n",
    "        class_counts[most_common_class] -= len(classes)  # Reduce count\n",
    "\n",
    "        # Stop removing once threshold is met\n",
    "        if class_counts[most_common_class] <= threshold:\n",
    "            break\n",
    "\n",
    "print(f\"\\nRemoved {removed_files} images where only class {most_common_class} was present.\")\n",
    "print(f\"Final count of class {most_common_class}: {class_counts[most_common_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset: 23736 labeled, 48503 unlabeled\n",
      "Removing 38331 unlabeled images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing Unlabeled Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38331/38331 [00:07<00:00, 4878.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset: 23736 labeled, 10172 unlabeled\n",
      "ðŸŽ¯ Achieved Ratio: 70.00% labeled, 30.00% unlabeled\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "images_folder = \"datasets/images_filtered\"\n",
    "labels_folder = \"datasets/labels_filtered\"\n",
    "\n",
    "# Get all image files\n",
    "image_files = [f for f in os.listdir(images_folder) if f.endswith(\".jpg\")]\n",
    "\n",
    "# Identify labeled images (those with a corresponding .txt file)\n",
    "labeled_images = {f.replace(\".jpg\", \".txt\") for f in image_files if os.path.exists(os.path.join(labels_folder, f.replace(\".jpg\", \".txt\")))}\n",
    "unlabeled_images = [f for f in image_files if f.replace(\".jpg\", \".txt\") not in labeled_images]\n",
    "\n",
    "# Dataset statistics\n",
    "total_labeled = len(labeled_images)\n",
    "total_unlabeled = len(unlabeled_images)\n",
    "\n",
    "print(f\"Initial dataset: {total_labeled} labeled, {total_unlabeled} unlabeled\")\n",
    "\n",
    "# Target number of unlabeled images for a 70:30 ratio\n",
    "target_unlabeled = int((total_labeled / 0.7) * 0.3)\n",
    "\n",
    "# Remove excess unlabeled images if needed\n",
    "if total_unlabeled > target_unlabeled:\n",
    "    images_to_remove = total_unlabeled - target_unlabeled\n",
    "    print(f\"Removing {images_to_remove} unlabeled images...\")\n",
    "\n",
    "    for image_file in tqdm(random.sample(unlabeled_images, images_to_remove), desc=\"Removing Unlabeled Images\"):\n",
    "        os.remove(os.path.join(images_folder, image_file))\n",
    "\n",
    "# Final dataset check\n",
    "final_total_images = len(os.listdir(images_folder))\n",
    "final_labeled = len([f for f in os.listdir(labels_folder) if f.endswith(\".txt\")])\n",
    "final_unlabeled = final_total_images - final_labeled\n",
    "\n",
    "print(f\"Final dataset: {final_labeled} labeled, {final_unlabeled} unlabeled\")\n",
    "print(f\"ðŸŽ¯ Achieved Ratio: {final_labeled / final_total_images:.2%} labeled, {final_unlabeled / final_total_images:.2%} unlabeled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train, Validation and Test image sets\n",
    "From the image and labels (\"dataset/images\", \"dataset/labels_filtered\")\n",
    "Create the test, validation and test sets.\n",
    "\n",
    "\n",
    "- Training is stored in (\"dataset/train/images\", \"dataset/train/labels\")\n",
    "- Validation is stored in (\"dataset/valid/images\", \"dataset/valid/labels\")\n",
    "- Test is stored in (\"dataset/test/images\", \"dataset/test/labels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code takes the unfiltered images and create the corresponding training, validation and test set in the following folders:\n",
    "- dataset/train\n",
    "- dataset/valid\n",
    "- dataset/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import random\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Split data into 20% train, 5% validation, and 5% test\n",
    "# train_perc = 0.8\n",
    "# valid_perc = 0.1\n",
    "# test_perc = 0.1\n",
    "\n",
    "# # Define folder paths\n",
    "# images_folder = \"datasets/images\"  # Folder containing filtered images\n",
    "# labels_folder = \"datasets/labels\"  # Folder containing filtered labels\n",
    "\n",
    "# train_images_folder = \"datasets/train/images\"\n",
    "# train_labels_folder = \"datasets/train/labels\"\n",
    "\n",
    "# valid_images_folder = \"datasets/valid/images\"\n",
    "# valid_labels_folder = \"datasets/valid/labels\"\n",
    "\n",
    "# test_images_folder = \"datasets/test/images\"\n",
    "# test_labels_folder = \"datasets/test/labels\"\n",
    "\n",
    "# # Create output directories\n",
    "# for folder in [train_images_folder, train_labels_folder,\n",
    "#                valid_images_folder, valid_labels_folder,\n",
    "#                test_images_folder, test_labels_folder]:\n",
    "#     if os.path.exists(folder):\n",
    "#         # Remove the folder and its contents\n",
    "#         shutil.rmtree(folder, ignore_errors=True)\n",
    "    \n",
    "#     os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# # Get a list of all images\n",
    "# image_files = sorted(os.listdir(images_folder))\n",
    "\n",
    "# # Create a list of images with and without labels\n",
    "# data = []\n",
    "# for image_file in image_files:\n",
    "#     label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "#     if os.path.exists(os.path.join(labels_folder, label_file)):\n",
    "#         data.append((image_file, label_file))  # Image has a corresponding label\n",
    "#     else:\n",
    "#         data.append((image_file, None))  # Image has no label (no objects detected)\n",
    "\n",
    "# # Shuffle the data\n",
    "# random.shuffle(data)\n",
    "\n",
    "# # Calculate splits\n",
    "# total_data = len(data)\n",
    "# train_split = int(train_perc * total_data)\n",
    "# valid_split = train_split + int(valid_perc * total_data)\n",
    "# test_split = valid_split + int(test_perc * total_data)\n",
    "\n",
    "# # Allocate data\n",
    "# train_data = data[:train_split]\n",
    "# valid_data = data[train_split:valid_split]\n",
    "# test_data = data[valid_split:test_split]\n",
    "\n",
    "# # Function to copy images and labels with a progress bar\n",
    "# def copy_files(data, dest_images_folder, dest_labels_folder, phase_name):\n",
    "#     with tqdm(total=len(data), desc=f\"Copying {phase_name}\") as pbar:\n",
    "#         for image_file, label_file in data:\n",
    "#             # Copy the image file\n",
    "#             shutil.copy(os.path.join(images_folder, image_file), os.path.join(dest_images_folder, image_file))\n",
    "#             # Copy the label file if it exists\n",
    "#             if label_file:\n",
    "#                 shutil.copy(os.path.join(labels_folder, label_file), os.path.join(dest_labels_folder, label_file))\n",
    "#             # Update progress bar\n",
    "#             pbar.update(1)\n",
    "\n",
    "# # Copy data to respective folders\n",
    "# copy_files(train_data, train_images_folder, train_labels_folder, \"Training Data\")\n",
    "# copy_files(valid_data, valid_images_folder, valid_labels_folder, \"Validation Data\")\n",
    "# copy_files(test_data, test_images_folder, test_labels_folder, \"Testing Data\")\n",
    "\n",
    "# print(\"Dataset split complete!\")\n",
    "# print(f\"Training data: {len(train_data)} images\")\n",
    "# print(f\"Validation data: {len(valid_data)} images\")\n",
    "# print(f\"Testing data: {len(test_data)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code takes the filtered images and create the corresponding training, validation and test set in the following folders:\n",
    "- dataset/train_filtered\n",
    "- dataset/valid_filtered\n",
    "- dataset/test_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying Training Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27126/27126 [03:22<00:00, 133.90it/s]\n",
      "Copying Validation Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3390/3390 [00:28<00:00, 119.48it/s]\n",
      "Copying Testing Data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3390/3390 [00:36<00:00, 91.87it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split complete!\n",
      "Training data: 27126 images\n",
      "Validation data: 3390 images\n",
      "Testing data: 3390 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Split data into 20% train, 5% validation, and 5% test\n",
    "train_perc = 0.8\n",
    "valid_perc = 0.1\n",
    "test_perc = 0.1\n",
    "\n",
    "# Define folder paths\n",
    "images_folder = \"datasets/images_filtered\"  # Folder containing filtered images\n",
    "labels_folder = \"datasets/labels_filtered\"  # Folder containing filtered labels\n",
    "\n",
    "train_images_folder = \"datasets/train_filtered/images\"\n",
    "train_labels_folder = \"datasets/train_filtered/labels\"\n",
    "\n",
    "valid_images_folder = \"datasets/valid_filtered/images\"\n",
    "valid_labels_folder = \"datasets/valid_filtered/labels\"\n",
    "\n",
    "test_images_folder = \"datasets/test_filtered/images\"\n",
    "test_labels_folder = \"datasets/test_filtered/labels\"\n",
    "\n",
    "# Create output directories\n",
    "for folder in [train_images_folder, train_labels_folder,\n",
    "               valid_images_folder, valid_labels_folder,\n",
    "               test_images_folder, test_labels_folder]:\n",
    "    if os.path.exists(folder):\n",
    "        # Remove the folder and its contents\n",
    "        shutil.rmtree(folder, ignore_errors=True)\n",
    "    \n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Get a list of all images\n",
    "image_files = sorted(os.listdir(images_folder))\n",
    "\n",
    "# Create a list of images with and without labels\n",
    "data = []\n",
    "for image_file in image_files:\n",
    "    label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "    if os.path.exists(os.path.join(labels_folder, label_file)):\n",
    "        data.append((image_file, label_file))  # Image has a corresponding label\n",
    "    else:\n",
    "        data.append((image_file, None))  # Image has no label (no objects detected)\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate splits\n",
    "total_data = len(data)\n",
    "train_split = int(train_perc * total_data)\n",
    "valid_split = train_split + int(valid_perc * total_data)\n",
    "test_split = valid_split + int(test_perc * total_data)\n",
    "\n",
    "# Allocate data\n",
    "train_data = data[:train_split]\n",
    "valid_data = data[train_split:valid_split]\n",
    "test_data = data[valid_split:test_split]\n",
    "\n",
    "# Function to copy images and labels with a progress bar\n",
    "def copy_files(data, dest_images_folder, dest_labels_folder, phase_name):\n",
    "    with tqdm(total=len(data), desc=f\"Copying {phase_name}\") as pbar:\n",
    "        for image_file, label_file in data:\n",
    "            # Copy the image file\n",
    "            shutil.copy(os.path.join(images_folder, image_file), os.path.join(dest_images_folder, image_file))\n",
    "            # Copy the label file if it exists\n",
    "            if label_file:\n",
    "                shutil.copy(os.path.join(labels_folder, label_file), os.path.join(dest_labels_folder, label_file))\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "# Copy data to respective folders\n",
    "copy_files(train_data, train_images_folder, train_labels_folder, \"Training Data\")\n",
    "copy_files(valid_data, valid_images_folder, valid_labels_folder, \"Validation Data\")\n",
    "copy_files(test_data, test_images_folder, test_labels_folder, \"Testing Data\")\n",
    "\n",
    "print(\"Dataset split complete!\")\n",
    "print(f\"Training data: {len(train_data)} images\")\n",
    "print(f\"Validation data: {len(valid_data)} images\")\n",
    "print(f\"Testing data: {len(test_data)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets/labels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122218/122218 [09:34<00:00, 212.72file/s]\n",
      "Processing datasets/labels_filtered: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23736/23736 [00:31<00:00, 755.60file/s] \n",
      "Processing datasets/train_filtered/labels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19000/19000 [01:30<00:00, 210.95file/s]\n",
      "Processing datasets/valid_filtered/labels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2341/2341 [00:10<00:00, 219.71file/s]\n",
      "Processing datasets/test_filtered/labels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2393/2393 [00:10<00:00, 222.33file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class counts per directory:\n",
      "\n",
      "Directory: datasets/labels\n",
      "  Class 0: 268029\n",
      "  Class 1: 7370\n",
      "  Class 2: 45451\n",
      "  Class 3: 9021\n",
      "  Class 4: 5272\n",
      "  Class 5: 6344\n",
      "  Class 6: 4760\n",
      "  Class 7: 10384\n",
      "  Class 8: 11000\n",
      "  Class 9: 13476\n",
      "  Class 10: 1966\n",
      "  Class 11: 2058\n",
      "  Class 12: 1343\n",
      "  Class 13: 10231\n",
      "  Class 14: 10969\n",
      "  Class 15: 4968\n",
      "  Class 16: 5718\n",
      "  Class 17: 6839\n",
      "  Class 18: 9577\n",
      "  Class 19: 8386\n",
      "  Class 20: 5736\n",
      "  Class 21: 1365\n",
      "  Class 22: 5535\n",
      "  Class 23: 5360\n",
      "  Class 24: 9085\n",
      "  Class 25: 11672\n",
      "  Class 26: 12882\n",
      "  Class 27: 6700\n",
      "  Class 28: 6411\n",
      "  Class 29: 2796\n",
      "  Class 30: 6864\n",
      "  Class 31: 2750\n",
      "  Class 32: 6559\n",
      "  Class 33: 9129\n",
      "  Class 34: 3418\n",
      "  Class 35: 3895\n",
      "  Class 36: 5715\n",
      "  Class 37: 6362\n",
      "  Class 38: 5032\n",
      "  Class 39: 25083\n",
      "  Class 40: 8180\n",
      "  Class 41: 21469\n",
      "  Class 42: 5689\n",
      "  Class 43: 8085\n",
      "  Class 44: 6412\n",
      "  Class 45: 14946\n",
      "  Class 46: 9565\n",
      "  Class 47: 6012\n",
      "  Class 48: 4533\n",
      "  Class 49: 6587\n",
      "  Class 50: 7573\n",
      "  Class 51: 8123\n",
      "  Class 52: 3008\n",
      "  Class 53: 6091\n",
      "  Class 54: 7333\n",
      "  Class 55: 6606\n",
      "  Class 56: 39844\n",
      "  Class 57: 6040\n",
      "  Class 58: 8973\n",
      "  Class 59: 4355\n",
      "  Class 60: 16390\n",
      "  Class 61: 4328\n",
      "  Class 62: 6091\n",
      "  Class 63: 5191\n",
      "  Class 64: 2367\n",
      "  Class 65: 5983\n",
      "  Class 66: 3007\n",
      "  Class 67: 6684\n",
      "  Class 68: 1727\n",
      "  Class 69: 3477\n",
      "  Class 70: 234\n",
      "  Class 71: 5834\n",
      "  Class 72: 2760\n",
      "  Class 73: 25206\n",
      "  Class 74: 6587\n",
      "  Class 75: 6851\n",
      "  Class 76: 1500\n",
      "  Class 77: 4919\n",
      "  Class 78: 209\n",
      "  Class 79: 2002\n",
      "\n",
      "Directory: datasets/labels_filtered\n",
      "  Class 0: 81116\n",
      "  Class 1: 7370\n",
      "  Class 2: 45451\n",
      "  Class 3: 9021\n",
      "  Class 4: 6344\n",
      "  Class 5: 4760\n",
      "  Class 6: 10384\n",
      "\n",
      "Directory: datasets/train_filtered/labels\n",
      "  Class 0: 65045\n",
      "  Class 1: 5880\n",
      "  Class 2: 36294\n",
      "  Class 3: 7307\n",
      "  Class 4: 5091\n",
      "  Class 5: 3796\n",
      "  Class 6: 8339\n",
      "\n",
      "Directory: datasets/valid_filtered/labels\n",
      "  Class 0: 7719\n",
      "  Class 1: 737\n",
      "  Class 2: 4494\n",
      "  Class 3: 814\n",
      "  Class 4: 592\n",
      "  Class 5: 471\n",
      "  Class 6: 1047\n",
      "\n",
      "Directory: datasets/test_filtered/labels\n",
      "  Class 0: 8349\n",
      "  Class 1: 753\n",
      "  Class 2: 4659\n",
      "  Class 3: 900\n",
      "  Class 4: 661\n",
      "  Class 5: 491\n",
      "  Class 6: 998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directories to process\n",
    "directories = [\n",
    "    \"datasets/labels\",\n",
    "    # \"datasets/train/labels\",\n",
    "    # \"datasets/valid/labels\",\n",
    "    # \"datasets/test/labels\",\n",
    "    \n",
    "    \"datasets/labels_filtered\",\n",
    "    \"datasets/train_filtered/labels\",\n",
    "    \"datasets/valid_filtered/labels\",\n",
    "    \"datasets/test_filtered/labels\",\n",
    "]\n",
    "\n",
    "def count_labels_in_directory(directory):\n",
    "    \"\"\"\n",
    "    Counts the number of each class in a directory of YOLO label files with a progress bar.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing label files.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with class IDs as keys and counts as values.\n",
    "    \"\"\"\n",
    "    class_counts = defaultdict(int)\n",
    "    \n",
    "    # Get all .txt files in the directory\n",
    "    label_files = [f for f in os.listdir(directory) if f.endswith(\".txt\")]\n",
    "    \n",
    "    # Process each file with a progress bar\n",
    "    with tqdm(total=len(label_files), desc=f\"Processing {directory}\", unit=\"file\") as pbar:\n",
    "        for label_file in label_files:\n",
    "            label_path = os.path.join(directory, label_file)\n",
    "            with open(label_path, \"r\") as file:\n",
    "                for line in file:\n",
    "                    parts = line.split()\n",
    "                    class_id = int(parts[0])  # Extract class ID\n",
    "                    class_counts[class_id] += 1  # Increment the count for the class ID\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "# Count labels for each directory\n",
    "results = {}\n",
    "\n",
    "for directory in directories:\n",
    "    if os.path.exists(directory):\n",
    "        results[directory] = count_labels_in_directory(directory)\n",
    "    else:\n",
    "        results[directory] = None  # Directory does not exist\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nClass counts per directory:\")\n",
    "for directory, class_counts in results.items():\n",
    "    print(f\"\\nDirectory: {directory}\")\n",
    "    if class_counts is not None:\n",
    "        for class_id, count in sorted(class_counts.items()):\n",
    "            print(f\"  Class {class_id}: {count}\")\n",
    "    else:\n",
    "        print(\"  Directory does not exist or contains no labels.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load YOLO Model and Begin Training!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\n",
      "\u001b[2K\n",
      "Ultralytics 8.3.78 ðŸš€ Python-3.11.9 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "Setup complete âœ… (22 CPUs, 31.7 GB RAM, 591.4/921.8 GB disk)\n",
      "\n",
      "OS                  Windows-10-10.0.26100-SP0\n",
      "Environment         Windows\n",
      "Python              3.11.9\n",
      "Install             pip\n",
      "RAM                 31.67 GB\n",
      "Disk                591.4/921.8 GB\n",
      "CPU                 Intel Core(TM) Ultra 9 185H\n",
      "CPU count           22\n",
      "GPU                 NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB\n",
      "GPU count           1\n",
      "CUDA                12.4\n",
      "\n",
      "numpy               âœ… 1.26.3<=2.1.1,>=1.23.0\n",
      "matplotlib          âœ… 3.10.0>=3.3.0\n",
      "opencv-python       âœ… 4.11.0.86>=4.6.0\n",
      "pillow              âœ… 10.2.0>=7.1.2\n",
      "pyyaml              âœ… 6.0.2>=5.3.1\n",
      "requests            âœ… 2.32.3>=2.23.0\n",
      "scipy               âœ… 1.15.1>=1.4.1\n",
      "torch               âœ… 2.5.1+cu124>=1.8.0\n",
      "torch               âœ… 2.5.1+cu124!=2.4.0,>=1.8.0; sys_platform == \"win32\"\n",
      "torchvision         âœ… 0.20.1+cu124>=0.9.0\n",
      "tqdm                âœ… 4.67.1>=4.64.0\n",
      "psutil              âœ… 6.1.1\n",
      "py-cpuinfo          âœ… 9.0.0\n",
      "pandas              âœ… 2.2.3>=1.1.4\n",
      "seaborn             âœ… 0.13.2>=0.11.0\n",
      "ultralytics-thop    âœ… 2.0.14>=2.0.0\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "from ultralytics import YOLO\n",
    "!yolo checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "# model = YOLO(\"yolo11n.yaml\")\n",
    "# model.tune(\n",
    "#     data=\"train_filtered.yaml\",\n",
    "#     project=\"./hyperparam_tuning3\",\n",
    "#     pretrained=False,  \n",
    "#     epochs = 5,\n",
    "#     iterations=100,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with base hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.82 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.78  Python-3.11.9 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.yaml, data=train_filtered.yaml, epochs=200, time=None, patience=10, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=models/base_16, name=train3, exist_ok=False, pretrained=False, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=models\\base_16\\train3\n",
      "Overriding model.yaml nc=80 with nc=7\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    432037  ultralytics.nn.modules.head.Detect           [7, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,591,205 parameters, 2,591,189 gradients, 6.4 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir models\\base_16\\train3', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Usuario\\Documents\\TFM\\ForeHelm\\training\\COCO\\datasets\\train_filtered\\labels.cache... 19000 images, 8126 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27126/27126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Usuario\\Documents\\TFM\\ForeHelm\\training\\COCO\\datasets\\train_filtered\\images\\000000099844.jpg: 2 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module '__main__' has no attribute '__spec__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolo11n.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_filtered.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/base_16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\engine\\model.py:810\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\engine\\trainer.py:208\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\engine\\trainer.py:323\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m world_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_ddp(world_size)\n\u001b[1;32m--> 323\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m nb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)  \u001b[38;5;66;03m# number of batches\u001b[39;00m\n\u001b[0;32m    326\u001b[0m nw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mwarmup_epochs \u001b[38;5;241m*\u001b[39m nb), \u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mwarmup_epochs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# warmup iterations\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\engine\\trainer.py:287\u001b[0m, in \u001b[0;36mBaseTrainer._setup_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m# Dataloaders\u001b[39;00m\n\u001b[0;32m    286\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(world_size, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOCAL_RANK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# Note: When training DOTA dataset, double batch size could get OOM on images with >2000 objects.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dataloader(\n\u001b[0;32m    291\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\models\\yolo\\detect\\train.py:55\u001b[0m, in \u001b[0;36mDetectionTrainer.get_dataloader\u001b[1;34m(self, dataset_path, batch_size, rank, mode)\u001b[0m\n\u001b[0;32m     53\u001b[0m     shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     54\u001b[0m workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\data\\build.py:147\u001b[0m, in \u001b[0;36mbuild_dataloader\u001b[1;34m(dataset, batch, workers, shuffle, rank)\u001b[0m\n\u001b[0;32m    145\u001b[0m generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mGenerator()\n\u001b[0;32m    146\u001b[0m generator\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m6148914691236517205\u001b[39m \u001b[38;5;241m+\u001b[39m RANK)\n\u001b[1;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInfiniteDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIN_MEMORY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollate_fn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker_init_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\data\\build.py:39\u001b[0m, in \u001b[0;36mInfiniteDataLoader.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_sampler\u001b[39m\u001b[38;5;124m\"\u001b[39m, _RepeatSampler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler))\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__iter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\popen_spawn_win32.py:46\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m---> 46\u001b[0m     prep_data \u001b[38;5;241m=\u001b[39m \u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_preparation_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# read end of pipe will be duplicated by the child process\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# -- see spawn_main() in spawn.py.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# bpo-33929: Previously, the read end of pipe was \"stolen\" by the child\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# process, but it leaked a handle if the child process had been\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# terminated before it could steal the handle from the parent process.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     rhandle, whandle \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mCreatePipe(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\spawn.py:193\u001b[0m, in \u001b[0;36mget_preparation_data\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Figure out whether to initialise main in the subprocess as a module\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# or through direct execution (or to leave it alone entirely)\u001b[39;00m\n\u001b[0;32m    192\u001b[0m main_module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 193\u001b[0m main_mod_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mmain_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__spec__\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m main_mod_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_main_from_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m main_mod_name\n",
      "\u001b[1;31mAttributeError\u001b[0m: module '__main__' has no attribute '__spec__'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo11n.yaml\")\n",
    "model.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    project=\"models/base_16\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model32 = YOLO(\"yolo11n.yaml\")\n",
    "model32.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    project=\"models/base_32\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "modelX = YOLO(\"yolo11n.yaml\")\n",
    "modelX.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    project=\"models/base_X\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with tuned 1 hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model16 = YOLO(\"yolo11n.yaml\")\n",
    "model16.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    cfg=\"tunings/tuning1/tune/best_hyperparameters.yaml\",\n",
    "    project=\"models/tuned1_16\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model32 = YOLO(\"yolo11n.yaml\")\n",
    "model32.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    cfg=\"tunings/tuning1/tune/best_hyperparameters.yaml\",\n",
    "    project=\"models/tuned1_32\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.82 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.78  Python-3.11.9 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.yaml, data=train_filtered.yaml, epochs=200, time=None, patience=10, batch=-1, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=models/tuned1_X, name=train3, exist_ok=False, pretrained=False, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.00938, lrf=0.01068, momentum=0.89665, weight_decay=0.00049, warmup_epochs=2.56713, warmup_momentum=0.37142, warmup_bias_lr=0.1, box=7.93555, cls=0.86648, dfl=1.88871, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.01108, hsv_s=0.84375, hsv_v=0.27844, degrees=0.0, translate=0.11442, scale=0.35418, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.63681, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=tunings/tuning1/tune/best_hyperparameters.yaml, tracker=botsort.yaml, save_dir=models\\tuned1_X\\train3\n",
      "Overriding model.yaml nc=80 with nc=7\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    432037  ultralytics.nn.modules.head.Detect           [7, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,591,205 parameters, 2,591,189 gradients, 6.4 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir models\\tuned1_X\\train3', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Usuario\\Documents\\TFM\\ForeHelm\\training\\COCO\\datasets\\train_filtered\\labels.cache... 19000 images, 8126 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27126/27126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Usuario\\Documents\\TFM\\ForeHelm\\training\\COCO\\datasets\\train_filtered\\images\\000000099844.jpg: 2 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=640 at 60.0% CUDA memory utilization.\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU) 8.00G total, 0.26G reserved, 0.09G allocated, 7.64G free\n",
      "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n",
      "     2591205       6.447         0.461         53.86         66.93        (1, 3, 640, 640)                    list\n",
      "     2591205       12.89         0.635         44.57         30.16        (2, 3, 640, 640)                    list\n",
      "     2591205       25.79         0.973         33.95         35.76        (4, 3, 640, 640)                    list\n",
      "     2591205       51.58         1.611         41.69         43.89        (8, 3, 640, 640)                    list\n",
      "     2591205       103.2         2.875         67.03         77.82       (16, 3, 640, 640)                    list\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 37 for CUDA:0 4.97G/8.00G (62%) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Usuario\\Documents\\TFM\\ForeHelm\\training\\COCO\\datasets\\train_filtered\\labels.cache... 19000 images, 8126 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27126/27126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Usuario\\Documents\\TFM\\ForeHelm\\training\\COCO\\datasets\\train_filtered\\images\\000000099844.jpg: 2 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module '__main__' has no attribute '__spec__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[0;32m      3\u001b[0m modelX \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolo11n.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodelX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_filtered.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtunings/tuning1/tune/best_hyperparameters.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/tuned1_X\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\engine\\model.py:810\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\engine\\trainer.py:208\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\engine\\trainer.py:323\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m world_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_ddp(world_size)\n\u001b[1;32m--> 323\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m nb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)  \u001b[38;5;66;03m# number of batches\u001b[39;00m\n\u001b[0;32m    326\u001b[0m nw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mwarmup_epochs \u001b[38;5;241m*\u001b[39m nb), \u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mwarmup_epochs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# warmup iterations\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\engine\\trainer.py:287\u001b[0m, in \u001b[0;36mBaseTrainer._setup_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m# Dataloaders\u001b[39;00m\n\u001b[0;32m    286\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(world_size, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOCAL_RANK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# Note: When training DOTA dataset, double batch size could get OOM on images with >2000 objects.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dataloader(\n\u001b[0;32m    291\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\models\\yolo\\detect\\train.py:55\u001b[0m, in \u001b[0;36mDetectionTrainer.get_dataloader\u001b[1;34m(self, dataset_path, batch_size, rank, mode)\u001b[0m\n\u001b[0;32m     53\u001b[0m     shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     54\u001b[0m workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\data\\build.py:147\u001b[0m, in \u001b[0;36mbuild_dataloader\u001b[1;34m(dataset, batch, workers, shuffle, rank)\u001b[0m\n\u001b[0;32m    145\u001b[0m generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mGenerator()\n\u001b[0;32m    146\u001b[0m generator\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m6148914691236517205\u001b[39m \u001b[38;5;241m+\u001b[39m RANK)\n\u001b[1;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInfiniteDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIN_MEMORY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollate_fn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker_init_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ultralytics\\data\\build.py:39\u001b[0m, in \u001b[0;36mInfiniteDataLoader.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_sampler\u001b[39m\u001b[38;5;124m\"\u001b[39m, _RepeatSampler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler))\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__iter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\popen_spawn_win32.py:46\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m---> 46\u001b[0m     prep_data \u001b[38;5;241m=\u001b[39m \u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_preparation_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# read end of pipe will be duplicated by the child process\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# -- see spawn_main() in spawn.py.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# bpo-33929: Previously, the read end of pipe was \"stolen\" by the child\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# process, but it leaked a handle if the child process had been\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# terminated before it could steal the handle from the parent process.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     rhandle, whandle \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mCreatePipe(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\spawn.py:193\u001b[0m, in \u001b[0;36mget_preparation_data\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Figure out whether to initialise main in the subprocess as a module\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# or through direct execution (or to leave it alone entirely)\u001b[39;00m\n\u001b[0;32m    192\u001b[0m main_module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 193\u001b[0m main_mod_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mmain_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__spec__\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m main_mod_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_main_from_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m main_mod_name\n",
      "\u001b[1;31mAttributeError\u001b[0m: module '__main__' has no attribute '__spec__'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "modelX = YOLO(\"yolo11n.yaml\")\n",
    "modelX.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    cfg=\"tunings/tuning1/tune/best_hyperparameters.yaml\",\n",
    "    project=\"models/tuned1_X\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with tuned 2 hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model16 = YOLO(\"yolo11n.yaml\")\n",
    "model16.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    cfg=\"tunings/tuning2/tune/best_hyperparameters.yaml\",\n",
    "    project=\"models/tuned2_16\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model32 = YOLO(\"yolo11n.yaml\")\n",
    "model32.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    cfg=\"tunings/tuning2/tune/best_hyperparameters.yaml\",\n",
    "    project=\"models/tuned2_32\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "modelX = YOLO(\"yolo11n.yaml\")\n",
    "modelX.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    cfg=\"tunings/tuning2/tune/best_hyperparameters.yaml\",\n",
    "    project=\"models/tuned2_X\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model16 = YOLO(\"models/base_16/train/weights/last.pt\")\n",
    "model16.train(resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the YOLO models\n",
    "The following code is used to validate and compare the different YOLO models that have been trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code tries the default and pretrained yolo11n model with the COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# List of model weight paths\n",
    "weights_paths = [\n",
    "    \"yolo11n.pt\",\n",
    "    \"models/base_16/train/weights/best.pt\",\n",
    "    \"models/base_32/train/weights/best.pt\",\n",
    "    \"models/base_X/train/weights/best.pt\",\n",
    "    \"models/tuned1_16/train/weights/best.pt\",\n",
    "    \"models/tuned1_32/train2/weights/best.pt\",\n",
    "    \"models/tuned21_X/train/weights/best.pt\",\n",
    "    \"models/tuned2_16/train/weights/best.pt\",\n",
    "    \"models/tuned2_32/train/weights/best.pt\",\n",
    "    \"models/tuned2_X/train/weights/best.pt\"\n",
    "]\n",
    "\n",
    "# Corresponding project names for each model\n",
    "prj_paths = [\n",
    "    \"yolo11n\",\n",
    "    \"models/base_16\",\n",
    "    \"models/base_32\",\n",
    "    \"models/base_X\",\n",
    "    \"models/tuned1_16\",\n",
    "    \"models/tuned1_32\",\n",
    "    \"models/tuned21_X\",\n",
    "    \"models/tuned2_16\",\n",
    "    \"models/tuned2_32\",\n",
    "    \"models/tuned2_X\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your dataset configuration file\n",
    "data_path = \"train_filtered.yaml\"\n",
    "# Device to run the validation on (e.g., '0' for the first GPU, 'cpu' for CPU)\n",
    "device = \"0\"\n",
    "# Dictionary to store validation results\n",
    "validation_results = {}\n",
    "\n",
    "# Iterate over each model weight path and its corresponding project name\n",
    "for weights_path, prj_path in zip(weights_paths, prj_paths):\n",
    "    # Load the model\n",
    "    model = YOLO(weights_path)    \n",
    "    # Perform validation\n",
    "    results = model.val(data=data_path, project=prj_path, device=device, save_json=True, save=False)\n",
    "    # Extract relevant metrics\n",
    "    metrics = {\n",
    "        'mAP50': results.box.map50,\n",
    "        'mAP50-95': results.box.map,\n",
    "        'mAP75': results.box.map75,\n",
    "        'mAPs': results.box.maps\n",
    "    }\n",
    "    # Store metrics in the dictionary\n",
    "    validation_results[prj_path] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.benchmarks import benchmark\n",
    "benchmark_results = {}\n",
    "for weights_path, prj_path in zip(weights_paths, prj_paths):\n",
    "    # Benchmark specific export format\n",
    "    benchmark_results[prj_path] = benchmark(model=weights_path, data=data_path, imgsz=640, format=\"ncnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "for weights_path in weights_paths:\n",
    "    # Load the model\n",
    "    model = YOLO(weights_path)\n",
    "    # Export the model to NCNN format\n",
    "    model.export(format=\"ncnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your dataset configuration file\n",
    "data_path = \"train_filtered.yaml\"\n",
    "# Device to run the validation on (e.g., '0' for the first GPU, 'cpu' for CPU)\n",
    "device = \"0\"\n",
    "# Dictionary to store validation results\n",
    "validation_results_ncnn = {}\n",
    "\n",
    "# Iterate over each model weight path and its corresponding project name\n",
    "for weights_path, prj_path in zip(weights_ncnn_paths, prj_paths):\n",
    "    # Load the model\n",
    "    model = YOLO(weights_path)\n",
    "    \n",
    "    # Perform validation\n",
    "    results = model.val(data=data_path, project=prj_path, name=\"val_ncnn\",device=device, save_json=True, save=False)\n",
    "    \n",
    "    # Extract relevant metrics\n",
    "    metrics = {\n",
    "        'mAP50': results.box.map50,\n",
    "        'mAP50-95': results.box.map,\n",
    "        'mAP75': results.box.map75,\n",
    "        'mAPs': results.box.maps\n",
    "    }\n",
    "    \n",
    "    # Store metrics in the dictionary\n",
    "    validation_results_ncnn[weights_path] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "def prune_model_l1(model, amount=0.2):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')\n",
    "    return model\n",
    "\n",
    "def prune_model_l2(model, amount=0.2):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.l2_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')\n",
    "    return model\n",
    "\n",
    "def prune_model_global(model, amount=0.2):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.global_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YOLO model\n",
    "weights = \"models/tuned1_16/train/weights/best.pt\"\n",
    "model = YOLO(weights)\n",
    "\n",
    "# Validate the original model (optional)\n",
    "# result = model.val(data=\"train_filtered.yaml\")\n",
    "print(\"Original model validation completed.\")\n",
    "torch_model = model.model\n",
    "print(torch_model)\n",
    "\n",
    "results_l1 = {}\n",
    "results_l2 = {}\n",
    "results_global = {}\n",
    "\n",
    "for i in np.arange(0.01, 0.21, 0.02):\n",
    "    # Prune the model by updating the model's internal torch model in place\n",
    "    print(\"Pruning model...\")\n",
    "    pruned_model = prune_model_l1(torch_model, amount=i)\n",
    "    print(f\"Model pruned for {i}.\")\n",
    "    model.model = pruned_model\n",
    "    # Validate the pruned model using the same YOLO validation method\n",
    "    result = model.val(data=\"train_filtered.yaml\")\n",
    "    results_l1[str(i)] =  result\n",
    "    print(f\"Pruned model validation completed for {i}.\")\n",
    "\n",
    "\n",
    "for i in np.arange(0.01, 0.21, 0.02):\n",
    "    # Prune the model by updating the model's internal torch model in place\n",
    "    print(\"Pruning model...\")\n",
    "    pruned_model = prune_model_l2(torch_model, amount=i)\n",
    "    print(f\"Model pruned for {i}.\")\n",
    "    model.model = pruned_model\n",
    "    # Validate the pruned model using the same YOLO validation method\n",
    "    result = model.val(data=\"train_filtered.yaml\")\n",
    "    results_l2[str(i)] =  result\n",
    "    print(f\"Pruned model validation completed for {i}.\")\n",
    "\n",
    "\n",
    "for i in np.arange(0.01, 0.21, 0.02):\n",
    "    # Prune the model by updating the model's internal torch model in place\n",
    "    print(\"Pruning model...\")\n",
    "    pruned_model = prune_model_global(torch_model, amount=i)\n",
    "    print(f\"Model pruned for {i}.\")\n",
    "    model.model = pruned_model\n",
    "    # Validate the pruned model using the same YOLO validation method\n",
    "    result = model.val(data=\"train_filtered.yaml\")\n",
    "    results_global[str(i)] =  result\n",
    "    print(f\"Pruned model validation completed for {i}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"base_40/train/weights/best.pt\")\n",
    "results = model.val(data=\"train_filtered.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
