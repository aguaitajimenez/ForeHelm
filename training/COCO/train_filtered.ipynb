{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This file will be used to format the training set for the different tests\n",
    "\n",
    "We will begin by clearing folders that have been created by this same script. So we can get a fresh setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# !pip install -U ultralytics\n",
    "import ultralytics\n",
    "# from ultralytics import settings\n",
    "# # Get the current working directory\n",
    "# datasets_folder = os.path.join(os.getcwd(), \"datasets\")\n",
    "# # Update the datasets_dir setting with the actual path value\n",
    "# if not os.path.exists(datasets_folder):\n",
    "#     os.makedirs(datasets_folder)\n",
    "# settings.update({\"datasets_dir\": datasets_folder})\n",
    "!yolo checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Path to the 'datasets' folder\n",
    "dataset_folder = \"datasets\"\n",
    "\n",
    "# List of folders to keep (Modify this if needed)\n",
    "folders_to_keep = []  # Example: [\"images\", \"labels\"]\n",
    "\n",
    "# Check if the dataset folder exists\n",
    "if not os.path.exists(dataset_folder):\n",
    "    print(f\"Error: The folder '{dataset_folder}' does not exist.\")\n",
    "else:\n",
    "    # Iterate through all items in the dataset folder\n",
    "    for item in os.listdir(dataset_folder):\n",
    "        item_path = os.path.join(dataset_folder, item)\n",
    "        \n",
    "        # Check if the item is not in the keep list\n",
    "        if item not in folders_to_keep:\n",
    "            # Remove the folder or file\n",
    "            if os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)  # Remove directories\n",
    "                print(f\"Removed folder: {item_path}\")\n",
    "            else:\n",
    "                os.remove(item_path)  # Remove files\n",
    "                print(f\"Removed file: {item_path}\")\n",
    "print(\"Cleanup completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code downloads the COCO dataset from github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.downloads import download\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the dataset folder path\n",
    "dataset_dir = Path().parent / \"datasets\"  # Set dataset path relative to the script\n",
    "\n",
    "# Ensure the dataset folder exists\n",
    "dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download labels\n",
    "segments = True  # segment or box labels\n",
    "url = 'https://github.com/ultralytics/assets/releases/download/v0.0.0/'\n",
    "urls = [url + ('coco2017labels-segments.zip' if segments else 'coco2017labels.zip')]  # labels\n",
    "download(urls, dir=dataset_dir / 'labels')\n",
    "\n",
    "# Download images\n",
    "urls = [\n",
    "    'http://images.cocodataset.org/zips/train2017.zip',  # 19G, 118k images\n",
    "    'http://images.cocodataset.org/zips/val2017.zip'  # 1G, 5k images\n",
    "    # ,'http://images.cocodataset.org/zips/test2017.zip'  # 7G, 41k images (optional)\n",
    "]\n",
    "download(urls, dir=dataset_dir / 'images', threads=3)\n",
    "\n",
    "print(f\"Labels saved to {dataset_dir / 'labels'}\")\n",
    "print(f\"Images saved to {dataset_dir / 'images'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image reordering\n",
    "Copies all the images from ~/datasets/coco into the /dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define source directories\n",
    "coco_train_images = os.path.expanduser(\"datasets/images/train2017\")\n",
    "coco_val_images = os.path.expanduser(\"datasets/images/val2017\")\n",
    "coco_train_labels = os.path.expanduser(\"datasets/labels/coco/labels/train2017\")\n",
    "coco_val_labels = os.path.expanduser(\"datasets/labels/coco/labels/val2017\")\n",
    "\n",
    "# Define target directories\n",
    "dataset_images = \"datasets/images\"\n",
    "dataset_labels = \"datasets/labels\"\n",
    "\n",
    "# Ensure target directories exist\n",
    "os.makedirs(dataset_images, exist_ok=True)\n",
    "os.makedirs(dataset_labels, exist_ok=True)\n",
    "\n",
    "# Helper function to copy files\n",
    "def copy_files(source_dir, target_dir, file_extension, phase_name):\n",
    "    \"\"\"\n",
    "    Copy files with a specific extension from source_dir to target_dir.\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): Source directory path.\n",
    "        target_dir (str): Target directory path.\n",
    "        file_extension (str): File extension to filter.\n",
    "        phase_name (str): Name of the phase (e.g., 'train', 'val') for progress.\n",
    "    \"\"\"\n",
    "    files = [f for f in os.listdir(source_dir) if f.endswith(file_extension)]\n",
    "    \n",
    "    with tqdm(total=len(files), desc=f\"Copying {phase_name} files\", unit=\"file\") as pbar:\n",
    "        for file_name in files:\n",
    "            src_path = os.path.relpath(os.path.join(source_dir, file_name))\n",
    "            dst_path = os.path.relpath(os.path.join(target_dir, file_name))\n",
    "            shutil.move(src_path, dst_path)\n",
    "            pbar.update(1)\n",
    "\n",
    "# Copy files\n",
    "copy_files(coco_train_images, dataset_images, \".jpg\", \"Training Images\")\n",
    "copy_files(coco_val_images, dataset_images, \".jpg\", \"Validation Images\")\n",
    "copy_files(coco_train_labels, dataset_labels, \".txt\", \"Training Labels\")\n",
    "copy_files(coco_val_labels, dataset_labels, \".txt\", \"Validation Labels\")\n",
    "\n",
    "print(\"Files successfully copied to dataset/images and dataset/labels with relative paths.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear unused files that have been downloaded from the COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directories to clean\n",
    "directories = [\n",
    "    \"datasets/images\",\n",
    "    \"datasets/labels\"\n",
    "]\n",
    "\n",
    "def clean_directory(directory, extensions=[\".jpg\", \".txt\"]):\n",
    "    \"\"\"\n",
    "    Removes all files and folders from a directory except those with the specified extensions.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): The directory to clean.\n",
    "        extensions (list): The file extensions to keep (default: [\".jpg\", \".txt\"]).\n",
    "    \"\"\"\n",
    "    for item in os.listdir(directory):\n",
    "        item_path = os.path.join(directory, item)\n",
    "        # Check if the item is a file and doesn't have a desired extension\n",
    "        if os.path.isfile(item_path) and not any(item.lower().endswith(ext) for ext in extensions):\n",
    "            os.remove(item_path)  # Remove the file\n",
    "        # If the item is a folder, remove it\n",
    "        elif os.path.isdir(item_path):\n",
    "            for root, dirs, files in os.walk(item_path, topdown=False):\n",
    "                for name in files:\n",
    "                    os.remove(os.path.join(root, name))\n",
    "                for name in dirs:\n",
    "                    os.rmdir(os.path.join(root, name))\n",
    "            os.rmdir(item_path)  # Remove the directory itself\n",
    "\n",
    "# Clean each directory\n",
    "for directory in directories:\n",
    "    if os.path.exists(directory):\n",
    "        clean_directory(directory)\n",
    "        print(f\"Cleaned directory: {directory}\")\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "\n",
    "print(\"Cleaning complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat dataset to include only vehicles.\n",
    "The original dataset contains the labels of:\n",
    "\n",
    "    [\"aeroplane\", \"bicyclebike\", \"bird\", \"boat\", \"bottle\", \"bus\",\n",
    "    \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\",\n",
    "    \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "A new filtered label directory has been created so that only remain the vehicles:\n",
    "\n",
    "    [\"car\", \"bus\", \"motorbike\", \"bicyclebike\"]\n",
    "\n",
    "These new labels are stored in dataset/labels_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing YOLO label .txt files\n",
    "label_dir = \"datasets/labels\"  # Replace with your label directory path\n",
    "\n",
    "# Allowed Class IDs for vehicle-related objects\n",
    "ALLOWED_CLASSES = {0, 1, 2, 3, 5, 6, 7}  # person, bicycle, car, motorcycle, bus, train, truck\n",
    "\n",
    "# Directory to save filtered labels\n",
    "output_dir = \"datasets/labels_filtered\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def filter_labels(label_file):\n",
    "    \"\"\"\n",
    "    Reads a YOLO label file, filters out unwanted classes,\n",
    "    and writes the remaining labels to a new file.\n",
    "    \"\"\"\n",
    "    input_path = os.path.join(label_dir, label_file)\n",
    "    output_path = os.path.join(output_dir, label_file)\n",
    "\n",
    "    with open(input_path, \"r\") as infile, open(output_path, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            parts = line.split()\n",
    "            class_id = int(parts[0])  # Extract class ID\n",
    "            if class_id in ALLOWED_CLASSES:\n",
    "                # Write the line if class ID is allowed\n",
    "                outfile.write(line)\n",
    "\n",
    "# List all .txt files in the label directory\n",
    "label_files = [f for f in os.listdir(label_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "# Process all .txt files with a progress bar\n",
    "with tqdm(total=len(label_files), desc=\"Filtering Labels\", unit=\"file\") as pbar:\n",
    "    for file_name in label_files:\n",
    "        filter_labels(file_name)\n",
    "        pbar.update(1)\n",
    "\n",
    "print(f\"Filtered labels saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove empty labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the labels folder\n",
    "labels_folder = \"datasets/labels_filtered\"\n",
    "\n",
    "# List all .txt files in the labels folder\n",
    "label_files = [f for f in os.listdir(labels_folder) if f.endswith('.txt')]\n",
    "\n",
    "# Initialize a counter for removed files\n",
    "removed_count = 0\n",
    "\n",
    "# Check each label file and remove it if it's empty\n",
    "for label_file in label_files:\n",
    "    label_path = os.path.join(labels_folder, label_file)\n",
    "    if os.path.getsize(label_path) == 0:  # Check if the file size is 0 bytes\n",
    "        os.remove(label_path)  # Remove the empty file\n",
    "        removed_count += 1\n",
    "        # print(f\"Removed empty label: {label_file}\")\n",
    "\n",
    "# Output the result\n",
    "print(f\"Total empty labels removed: {removed_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting images and filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "# Define the paths to the images and labels folders\n",
    "images_folder = \"datasets/images\"\n",
    "labels_folder = \"datasets/labels_filtered\"\n",
    "\n",
    "# List all files in the images and labels folders\n",
    "image_files = [f for f in os.listdir(images_folder) if f.endswith('.jpg')]\n",
    "label_files = [f for f in os.listdir(labels_folder) if f.endswith('.txt')]\n",
    "\n",
    "# Count the total number of images and labels\n",
    "num_images = len(image_files)\n",
    "num_labels = len(label_files)\n",
    "\n",
    "# Check for matching files (base filenames without extensions)\n",
    "image_basenames = {os.path.splitext(f)[0] for f in image_files}\n",
    "label_basenames = {os.path.splitext(f)[0] for f in label_files}\n",
    "\n",
    "# Count matched and unmatched files\n",
    "matched_files = image_basenames & label_basenames\n",
    "unmatched_images = image_basenames - label_basenames\n",
    "unmatched_labels = label_basenames - image_basenames\n",
    "\n",
    "print(f\"Total images: {num_images}\")\n",
    "print(f\"Total labels: {num_labels}\")\n",
    "print(f\"Matched files: {len(matched_files)}\")\n",
    "print(f\"Unmatched images: {len(unmatched_images)}\")\n",
    "print(f\"Unmatched labels: {len(unmatched_labels)}\")\n",
    "\n",
    "# Optionally print the unmatched files\n",
    "if unmatched_images:\n",
    "    print(\"Unmatched images (no corresponding label):\")\n",
    "    for img in unmatched_images:\n",
    "        print(f\"  {img}\")\n",
    "\n",
    "if unmatched_labels:\n",
    "    print(\"Unmatched labels (no corresponding image):\")\n",
    "    for lbl in unmatched_labels:\n",
    "        print(f\"  {lbl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the folder of images_filtered with a reduced number of unlabellel images.\n",
    "The ratio of labelled images and unlabelled images has been set to 50/50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Paths\n",
    "images_folder = \"datasets/images\"\n",
    "labels_folder = \"datasets/labels_filtered\"\n",
    "output_folder = \"datasets/images_filtered\"\n",
    "\n",
    "# Ratio of labeled and unlabeled images\n",
    "r_label = 50\n",
    "r_unlabel = 100 - r_label\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get all image files and corresponding label files\n",
    "image_files = [f for f in os.listdir(images_folder) if f.endswith('.jpg')]\n",
    "label_files = [f for f in os.listdir(labels_folder) if f.endswith('.txt')]\n",
    "\n",
    "# Get the base filenames (without extensions) for labels\n",
    "label_basenames = {os.path.splitext(label)[0] for label in label_files}\n",
    "\n",
    "# Separate labeled and unlabeled images\n",
    "labeled_images = [img for img in image_files if os.path.splitext(img)[0] in label_basenames]\n",
    "unlabeled_images = [img for img in image_files if os.path.splitext(img)[0] not in label_basenames]\n",
    "\n",
    "# Check counts\n",
    "num_labeled = len(labeled_images)\n",
    "num_unlabeled_to_select = min(int(num_labeled * r_unlabel / r_label), len(unlabeled_images))\n",
    "\n",
    "# Randomly select the required number of unlabeled images\n",
    "selected_unlabeled_images = random.sample(unlabeled_images, num_unlabeled_to_select)\n",
    "\n",
    "# Combine labeled and selected unlabeled images\n",
    "images_to_copy = labeled_images + selected_unlabeled_images\n",
    "\n",
    "# Copy labeled and selected unlabeled images to the output folder with a progress bar\n",
    "with tqdm(total=len(images_to_copy), desc=\"Copying Images\", unit=\"file\") as pbar:\n",
    "    for img in images_to_copy:\n",
    "        src_path = os.path.join(images_folder, img)\n",
    "        dst_path = os.path.join(output_folder, img)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Output results\n",
    "print(f\"Total labeled images: {num_labeled}\")\n",
    "print(f\"Total unlabeled images selected: {len(selected_unlabeled_images)}\")\n",
    "print(f\"Total images in 'images_filtered': {len(os.listdir(output_folder))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train, Validation and Test image sets\n",
    "From the image and labels (\"dataset/images\", \"dataset/labels_filtered\")\n",
    "Create the test, validation and test sets.\n",
    "\n",
    "\n",
    "- Training is stored in (\"dataset/train/images\", \"dataset/train/labels\")\n",
    "- Validation is stored in (\"dataset/valid/images\", \"dataset/valid/labels\")\n",
    "- Test is stored in (\"dataset/test/images\", \"dataset/test/labels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code takes the unfiltered images and create the corresponding training, validation and test set in the following folders:\n",
    "- dataset/train\n",
    "- dataset/valid\n",
    "- dataset/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Split data into 20% train, 5% validation, and 5% test\n",
    "train_perc = 0.8\n",
    "valid_perc = 0.1\n",
    "test_perc = 0.1\n",
    "\n",
    "# Define folder paths\n",
    "images_folder = \"datasets/images\"  # Folder containing filtered images\n",
    "labels_folder = \"datasets/labels\"  # Folder containing filtered labels\n",
    "\n",
    "train_images_folder = \"datasets/train/images\"\n",
    "train_labels_folder = \"datasets/train/labels\"\n",
    "\n",
    "valid_images_folder = \"datasets/valid/images\"\n",
    "valid_labels_folder = \"datasets/valid/labels\"\n",
    "\n",
    "test_images_folder = \"datasets/test/images\"\n",
    "test_labels_folder = \"datasets/test/labels\"\n",
    "\n",
    "# Create output directories\n",
    "for folder in [train_images_folder, train_labels_folder,\n",
    "               valid_images_folder, valid_labels_folder,\n",
    "               test_images_folder, test_labels_folder]:\n",
    "    if os.path.exists(folder):\n",
    "        # Remove the folder and its contents\n",
    "        shutil.rmtree(folder, ignore_errors=True)\n",
    "    \n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Get a list of all images\n",
    "image_files = sorted(os.listdir(images_folder))\n",
    "\n",
    "# Create a list of images with and without labels\n",
    "data = []\n",
    "for image_file in image_files:\n",
    "    label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "    if os.path.exists(os.path.join(labels_folder, label_file)):\n",
    "        data.append((image_file, label_file))  # Image has a corresponding label\n",
    "    else:\n",
    "        data.append((image_file, None))  # Image has no label (no objects detected)\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate splits\n",
    "total_data = len(data)\n",
    "train_split = int(train_perc * total_data)\n",
    "valid_split = train_split + int(valid_perc * total_data)\n",
    "test_split = valid_split + int(test_perc * total_data)\n",
    "\n",
    "# Allocate data\n",
    "train_data = data[:train_split]\n",
    "valid_data = data[train_split:valid_split]\n",
    "test_data = data[valid_split:test_split]\n",
    "\n",
    "# Function to copy images and labels with a progress bar\n",
    "def copy_files(data, dest_images_folder, dest_labels_folder, phase_name):\n",
    "    with tqdm(total=len(data), desc=f\"Copying {phase_name}\") as pbar:\n",
    "        for image_file, label_file in data:\n",
    "            # Copy the image file\n",
    "            shutil.copy(os.path.join(images_folder, image_file), os.path.join(dest_images_folder, image_file))\n",
    "            # Copy the label file if it exists\n",
    "            if label_file:\n",
    "                shutil.copy(os.path.join(labels_folder, label_file), os.path.join(dest_labels_folder, label_file))\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "# Copy data to respective folders\n",
    "copy_files(train_data, train_images_folder, train_labels_folder, \"Training Data\")\n",
    "copy_files(valid_data, valid_images_folder, valid_labels_folder, \"Validation Data\")\n",
    "copy_files(test_data, test_images_folder, test_labels_folder, \"Testing Data\")\n",
    "\n",
    "print(\"Dataset split complete!\")\n",
    "print(f\"Training data: {len(train_data)} images\")\n",
    "print(f\"Validation data: {len(valid_data)} images\")\n",
    "print(f\"Testing data: {len(test_data)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code takes the filtered images and create the corresponding training, validation and test set in the following folders:\n",
    "- dataset/train_filtered\n",
    "- dataset/valid_filtered\n",
    "- dataset/test_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Split data into 20% train, 5% validation, and 5% test\n",
    "train_perc = 0.8\n",
    "valid_perc = 0.1\n",
    "test_perc = 0.1\n",
    "\n",
    "# Define folder paths\n",
    "images_folder = \"datasets/images_filtered\"  # Folder containing filtered images\n",
    "labels_folder = \"datasets/labels_filtered\"  # Folder containing filtered labels\n",
    "\n",
    "train_images_folder = \"datasets/train_filtered/images\"\n",
    "train_labels_folder = \"datasets/train_filtered/labels\"\n",
    "\n",
    "valid_images_folder = \"datasets/valid_filtered/images\"\n",
    "valid_labels_folder = \"datasets/valid_filtered/labels\"\n",
    "\n",
    "test_images_folder = \"datasets/test_filtered/images\"\n",
    "test_labels_folder = \"datasets/test_filtered/labels\"\n",
    "\n",
    "# Create output directories\n",
    "for folder in [train_images_folder, train_labels_folder,\n",
    "               valid_images_folder, valid_labels_folder,\n",
    "               test_images_folder, test_labels_folder]:\n",
    "    if os.path.exists(folder):\n",
    "        # Remove the folder and its contents\n",
    "        shutil.rmtree(folder, ignore_errors=True)\n",
    "    \n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Get a list of all images\n",
    "image_files = sorted(os.listdir(images_folder))\n",
    "\n",
    "# Create a list of images with and without labels\n",
    "data = []\n",
    "for image_file in image_files:\n",
    "    label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "    if os.path.exists(os.path.join(labels_folder, label_file)):\n",
    "        data.append((image_file, label_file))  # Image has a corresponding label\n",
    "    else:\n",
    "        data.append((image_file, None))  # Image has no label (no objects detected)\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate splits\n",
    "total_data = len(data)\n",
    "train_split = int(train_perc * total_data)\n",
    "valid_split = train_split + int(valid_perc * total_data)\n",
    "test_split = valid_split + int(test_perc * total_data)\n",
    "\n",
    "# Allocate data\n",
    "train_data = data[:train_split]\n",
    "valid_data = data[train_split:valid_split]\n",
    "test_data = data[valid_split:test_split]\n",
    "\n",
    "# Function to copy images and labels with a progress bar\n",
    "def copy_files(data, dest_images_folder, dest_labels_folder, phase_name):\n",
    "    with tqdm(total=len(data), desc=f\"Copying {phase_name}\") as pbar:\n",
    "        for image_file, label_file in data:\n",
    "            # Copy the image file\n",
    "            shutil.copy(os.path.join(images_folder, image_file), os.path.join(dest_images_folder, image_file))\n",
    "            # Copy the label file if it exists\n",
    "            if label_file:\n",
    "                shutil.copy(os.path.join(labels_folder, label_file), os.path.join(dest_labels_folder, label_file))\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "# Copy data to respective folders\n",
    "copy_files(train_data, train_images_folder, train_labels_folder, \"Training Data\")\n",
    "copy_files(valid_data, valid_images_folder, valid_labels_folder, \"Validation Data\")\n",
    "copy_files(test_data, test_images_folder, test_labels_folder, \"Testing Data\")\n",
    "\n",
    "print(\"Dataset split complete!\")\n",
    "print(f\"Training data: {len(train_data)} images\")\n",
    "print(f\"Validation data: {len(valid_data)} images\")\n",
    "print(f\"Testing data: {len(test_data)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directories to process\n",
    "directories = [\n",
    "    \"datasets/labels\",\n",
    "    \"datasets/train/labels\",\n",
    "    \"datasets/valid/labels\",\n",
    "    \"datasets/test/labels\",\n",
    "    \n",
    "    \"datasets/labels_filtered\",\n",
    "    \"datasets/train_filtered/labels\",\n",
    "    \"datasets/valid_filtered/labels\",\n",
    "    \"datasets/test_filtered/labels\",\n",
    "]\n",
    "\n",
    "def count_labels_in_directory(directory):\n",
    "    \"\"\"\n",
    "    Counts the number of each class in a directory of YOLO label files with a progress bar.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing label files.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with class IDs as keys and counts as values.\n",
    "    \"\"\"\n",
    "    class_counts = defaultdict(int)\n",
    "    \n",
    "    # Get all .txt files in the directory\n",
    "    label_files = [f for f in os.listdir(directory) if f.endswith(\".txt\")]\n",
    "    \n",
    "    # Process each file with a progress bar\n",
    "    with tqdm(total=len(label_files), desc=f\"Processing {directory}\", unit=\"file\") as pbar:\n",
    "        for label_file in label_files:\n",
    "            label_path = os.path.join(directory, label_file)\n",
    "            with open(label_path, \"r\") as file:\n",
    "                for line in file:\n",
    "                    parts = line.split()\n",
    "                    class_id = int(parts[0])  # Extract class ID\n",
    "                    class_counts[class_id] += 1  # Increment the count for the class ID\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "# Count labels for each directory\n",
    "results = {}\n",
    "\n",
    "for directory in directories:\n",
    "    if os.path.exists(directory):\n",
    "        results[directory] = count_labels_in_directory(directory)\n",
    "    else:\n",
    "        results[directory] = None  # Directory does not exist\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nClass counts per directory:\")\n",
    "for directory, class_counts in results.items():\n",
    "    print(f\"\\nDirectory: {directory}\")\n",
    "    if class_counts is not None:\n",
    "        for class_id, count in sorted(class_counts.items()):\n",
    "            print(f\"  Class {class_id}: {count}\")\n",
    "    else:\n",
    "        print(\"  Directory does not exist or contains no labels.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load YOLO Model and Begin Training!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check for available devices\n",
    "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "available_devices = torch.cuda.device_count()\n",
    "\n",
    "print(f\"Device in use: {device_name}\")\n",
    "print(f\"Available CUDA devices: {available_devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "!yolo checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "# model = YOLO(\"yolo11n.yaml\")\n",
    "# model.tune(\n",
    "#     data=\"train_filtered.yaml\",\n",
    "#     project=\"./hyperparam_tuning3\",\n",
    "#     pretrained=False,  \n",
    "#     epochs = 5,\n",
    "#     iterations=100,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with base hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo11n.yaml\")\n",
    "model.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    project=\"models/base_16\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model32 = YOLO(\"yolo11n.yaml\")\n",
    "model32.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    project=\"models/base_32\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "modelX = YOLO(\"yolo11n.yaml\")\n",
    "modelX.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    project=\"models/base_X\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with tuned 1 hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model16 = YOLO(\"yolo11n.yaml\")\n",
    "model16.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    cfg=\"hyperparam_tuning1/tune/best_hyperparameters.yaml\",\n",
    "    project=\"models/tuned1_16\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model32 = YOLO(\"yolo11n.yaml\")\n",
    "model32.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    cfg=\"hyperparam_tuning1/tune/best_hyperparameters.yaml\",\n",
    "    project=\"models/tuned1_32\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "modelX = YOLO(\"yolo11n.yaml\")\n",
    "modelX.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    cfg=\"hyperparam_tuning1/tune/best_hyperparameters.yaml\",\n",
    "    project=\"models/tuned1_X\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with tuned 2 hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model16 = YOLO(\"yolo11n.yaml\")\n",
    "model16.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    cfg=\"hyperparam_tuning2/tune/best_hyperparameters.yaml\",\n",
    "    project=\"models/tuned2_16\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model32 = YOLO(\"yolo11n.yaml\")\n",
    "model32.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    cfg=\"hyperparam_tuning2/tune/best_hyperparameters.yaml\",\n",
    "    project=\"models/tuned2_32\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "modelX = YOLO(\"yolo11n.yaml\")\n",
    "modelX.train(\n",
    "    data=\"train_filtered.yaml\",\n",
    "    cfg=\"hyperparam_tuning2/tune/best_hyperparameters.yaml\",\n",
    "    project=\"models/tuned2_X\",\n",
    "    pretrained=False,  \n",
    "    epochs = 200,\n",
    "    patience=10, \n",
    "    batch=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model16 = YOLO(\"models/base_16/train/weights/last.pt\")\n",
    "model16.train(resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the YOLO models\n",
    "The following code is used to validate and compare the different YOLO models that have been trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code tries the default and pretrained yolo11n model with the COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# List of model weight paths\n",
    "weights_paths = [\n",
    "    \"yolo11n.pt\",\n",
    "    \"models/base_16/train/weights/best.pt\",\n",
    "    \"models/base_32/train/weights/best.pt\",\n",
    "    \"models/base_X/train/weights/best.pt\",\n",
    "    \"models/tuned1_16/train/weights/best.pt\",\n",
    "    \"models/tuned1_32/train2/weights/best.pt\",\n",
    "    \"models/tuned21_X/train/weights/best.pt\",\n",
    "    \"models/tuned2_16/train/weights/best.pt\",\n",
    "    \"models/tuned2_32/train/weights/best.pt\",\n",
    "    \"models/tuned2_X/train/weights/best.pt\"\n",
    "]\n",
    "\n",
    "# Corresponding project names for each model\n",
    "prj_paths = [\n",
    "    \"yolo11n\",\n",
    "    \"models/base_16\",\n",
    "    \"models/base_32\",\n",
    "    \"models/base_X\",\n",
    "    \"models/tuned1_16\",\n",
    "    \"models/tuned1_32\",\n",
    "    \"models/tuned21_X\",\n",
    "    \"models/tuned2_16\",\n",
    "    \"models/tuned2_32\",\n",
    "    \"models/tuned2_X\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your dataset configuration file\n",
    "data_path = \"train_filtered.yaml\"\n",
    "# Device to run the validation on (e.g., '0' for the first GPU, 'cpu' for CPU)\n",
    "device = \"0\"\n",
    "# Dictionary to store validation results\n",
    "validation_results = {}\n",
    "\n",
    "# Iterate over each model weight path and its corresponding project name\n",
    "for weights_path, prj_path in zip(weights_paths, prj_paths):\n",
    "    # Load the model\n",
    "    model = YOLO(weights_path)    \n",
    "    # Perform validation\n",
    "    results = model.val(data=data_path, project=prj_path, device=device, save_json=True, save=False)\n",
    "    # Extract relevant metrics\n",
    "    metrics = {\n",
    "        'mAP50': results.box.map50,\n",
    "        'mAP50-95': results.box.map,\n",
    "        'mAP75': results.box.map75,\n",
    "        'mAPs': results.box.maps\n",
    "    }\n",
    "    # Store metrics in the dictionary\n",
    "    validation_results[prj_path] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.benchmarks import benchmark\n",
    "benchmark_results = {}\n",
    "for weights_path, prj_path in zip(weights_paths, prj_paths):\n",
    "    # Benchmark specific export format\n",
    "    benchmark_results[prj_path] = benchmark(model=weights_path, data=data_path, imgsz=640, format=\"ncnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "for weights_path in weights_paths:\n",
    "    # Load the model\n",
    "    model = YOLO(weights_path)\n",
    "    # Export the model to NCNN format\n",
    "    model.export(format=\"ncnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your dataset configuration file\n",
    "data_path = \"train_filtered.yaml\"\n",
    "# Device to run the validation on (e.g., '0' for the first GPU, 'cpu' for CPU)\n",
    "device = \"0\"\n",
    "# Dictionary to store validation results\n",
    "validation_results_ncnn = {}\n",
    "\n",
    "# Iterate over each model weight path and its corresponding project name\n",
    "for weights_path, prj_path in zip(weights_ncnn_paths, prj_paths):\n",
    "    # Load the model\n",
    "    model = YOLO(weights_path)\n",
    "    \n",
    "    # Perform validation\n",
    "    results = model.val(data=data_path, project=prj_path, name=\"val_ncnn\",device=device, save_json=True, save=False)\n",
    "    \n",
    "    # Extract relevant metrics\n",
    "    metrics = {\n",
    "        'mAP50': results.box.map50,\n",
    "        'mAP50-95': results.box.map,\n",
    "        'mAP75': results.box.map75,\n",
    "        'mAPs': results.box.maps\n",
    "    }\n",
    "    \n",
    "    # Store metrics in the dictionary\n",
    "    validation_results_ncnn[weights_path] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "def prune_model_l1(model, amount=0.2):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')\n",
    "    return model\n",
    "\n",
    "def prune_model_l2(model, amount=0.2):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.l2_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')\n",
    "    return model\n",
    "\n",
    "def prune_model_global(model, amount=0.2):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.global_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YOLO model\n",
    "weights = \"models/tuned1_16/train/weights/best.pt\"\n",
    "model = YOLO(weights)\n",
    "\n",
    "# Validate the original model (optional)\n",
    "# result = model.val(data=\"train_filtered.yaml\")\n",
    "print(\"Original model validation completed.\")\n",
    "torch_model = model.model\n",
    "print(torch_model)\n",
    "\n",
    "results_l1 = {}\n",
    "results_l2 = {}\n",
    "results_global = {}\n",
    "\n",
    "for i in np.arange(0.01, 0.21, 0.02):\n",
    "    # Prune the model by updating the model's internal torch model in place\n",
    "    print(\"Pruning model...\")\n",
    "    pruned_model = prune_model_l1(torch_model, amount=i)\n",
    "    print(f\"Model pruned for {i}.\")\n",
    "    model.model = pruned_model\n",
    "    # Validate the pruned model using the same YOLO validation method\n",
    "    result = model.val(data=\"train_filtered.yaml\")\n",
    "    results_l1[str(i)] =  result\n",
    "    print(f\"Pruned model validation completed for {i}.\")\n",
    "\n",
    "\n",
    "for i in np.arange(0.01, 0.21, 0.02):\n",
    "    # Prune the model by updating the model's internal torch model in place\n",
    "    print(\"Pruning model...\")\n",
    "    pruned_model = prune_model_l2(torch_model, amount=i)\n",
    "    print(f\"Model pruned for {i}.\")\n",
    "    model.model = pruned_model\n",
    "    # Validate the pruned model using the same YOLO validation method\n",
    "    result = model.val(data=\"train_filtered.yaml\")\n",
    "    results_l2[str(i)] =  result\n",
    "    print(f\"Pruned model validation completed for {i}.\")\n",
    "\n",
    "\n",
    "for i in np.arange(0.01, 0.21, 0.02):\n",
    "    # Prune the model by updating the model's internal torch model in place\n",
    "    print(\"Pruning model...\")\n",
    "    pruned_model = prune_model_global(torch_model, amount=i)\n",
    "    print(f\"Model pruned for {i}.\")\n",
    "    model.model = pruned_model\n",
    "    # Validate the pruned model using the same YOLO validation method\n",
    "    result = model.val(data=\"train_filtered.yaml\")\n",
    "    results_global[str(i)] =  result\n",
    "    print(f\"Pruned model validation completed for {i}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"base_40/train/weights/best.pt\")\n",
    "results = model.val(data=\"train_filtered.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
